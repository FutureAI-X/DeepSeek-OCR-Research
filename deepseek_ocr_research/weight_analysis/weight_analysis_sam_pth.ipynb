{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f3d95be-2d14-486d-ae0f-57d42930614b",
   "metadata": {},
   "source": [
    "# 一 模型权重分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b49b26d9-2072-4056-bf8a-f542392d72af",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-06T01:22:08.383369Z",
     "iopub.status.busy": "2025-11-06T01:22:08.383142Z",
     "iopub.status.idle": "2025-11-06T01:22:08.385664Z",
     "shell.execute_reply": "2025-11-06T01:22:08.385221Z",
     "shell.execute_reply.started": "2025-11-06T01:22:08.383353Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f78b57ba-0c37-40a7-8607-346bf7605afd",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-06T01:22:10.139550Z",
     "iopub.status.busy": "2025-11-06T01:22:10.139346Z",
     "iopub.status.idle": "2025-11-06T01:22:10.141842Z",
     "shell.execute_reply": "2025-11-06T01:22:10.141406Z",
     "shell.execute_reply.started": "2025-11-06T01:22:10.139535Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_sam_pth = '/mnt/workspace/models/facebook/sam-vit-base/sam_vit_b_01ec64.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1bfec6d-ba62-49eb-9182-13e686d1cac1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-06T01:22:11.193543Z",
     "iopub.status.busy": "2025-11-06T01:22:11.193301Z",
     "iopub.status.idle": "2025-11-06T01:22:11.395876Z",
     "shell.execute_reply": "2025-11-06T01:22:11.395096Z",
     "shell.execute_reply.started": "2025-11-06T01:22:11.193525Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_dict_sam_pth = torch.load(checkpoint_sam_pth, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f0b138e-6903-4c03-a553-816a515b4d79",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-06T01:22:13.281364Z",
     "iopub.status.busy": "2025-11-06T01:22:13.281145Z",
     "iopub.status.idle": "2025-11-06T01:22:13.286964Z",
     "shell.execute_reply": "2025-11-06T01:22:13.286515Z",
     "shell.execute_reply.started": "2025-11-06T01:22:13.281347Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_encoder.blocks.0.attn.proj.bias, torch.Size([768])\n",
      "image_encoder.blocks.0.attn.proj.weight, torch.Size([768, 768])\n",
      "image_encoder.blocks.0.attn.qkv.bias, torch.Size([2304])\n",
      "image_encoder.blocks.0.attn.qkv.weight, torch.Size([2304, 768])\n",
      "image_encoder.blocks.0.attn.rel_pos_h, torch.Size([27, 64])\n",
      "image_encoder.blocks.0.attn.rel_pos_w, torch.Size([27, 64])\n",
      "image_encoder.blocks.0.mlp.lin1.bias, torch.Size([3072])\n",
      "image_encoder.blocks.0.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "image_encoder.blocks.0.mlp.lin2.bias, torch.Size([768])\n",
      "image_encoder.blocks.0.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "image_encoder.blocks.0.norm1.bias, torch.Size([768])\n",
      "image_encoder.blocks.0.norm1.weight, torch.Size([768])\n",
      "image_encoder.blocks.0.norm2.bias, torch.Size([768])\n",
      "image_encoder.blocks.0.norm2.weight, torch.Size([768])\n",
      "image_encoder.blocks.1.attn.proj.bias, torch.Size([768])\n",
      "image_encoder.blocks.1.attn.proj.weight, torch.Size([768, 768])\n",
      "image_encoder.blocks.1.attn.qkv.bias, torch.Size([2304])\n",
      "image_encoder.blocks.1.attn.qkv.weight, torch.Size([2304, 768])\n",
      "image_encoder.blocks.1.attn.rel_pos_h, torch.Size([27, 64])\n",
      "image_encoder.blocks.1.attn.rel_pos_w, torch.Size([27, 64])\n",
      "image_encoder.blocks.1.mlp.lin1.bias, torch.Size([3072])\n",
      "image_encoder.blocks.1.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "image_encoder.blocks.1.mlp.lin2.bias, torch.Size([768])\n",
      "image_encoder.blocks.1.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "image_encoder.blocks.1.norm1.bias, torch.Size([768])\n",
      "image_encoder.blocks.1.norm1.weight, torch.Size([768])\n",
      "image_encoder.blocks.1.norm2.bias, torch.Size([768])\n",
      "image_encoder.blocks.1.norm2.weight, torch.Size([768])\n",
      "image_encoder.blocks.10.attn.proj.bias, torch.Size([768])\n",
      "image_encoder.blocks.10.attn.proj.weight, torch.Size([768, 768])\n",
      "image_encoder.blocks.10.attn.qkv.bias, torch.Size([2304])\n",
      "image_encoder.blocks.10.attn.qkv.weight, torch.Size([2304, 768])\n",
      "image_encoder.blocks.10.attn.rel_pos_h, torch.Size([27, 64])\n",
      "image_encoder.blocks.10.attn.rel_pos_w, torch.Size([27, 64])\n",
      "image_encoder.blocks.10.mlp.lin1.bias, torch.Size([3072])\n",
      "image_encoder.blocks.10.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "image_encoder.blocks.10.mlp.lin2.bias, torch.Size([768])\n",
      "image_encoder.blocks.10.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "image_encoder.blocks.10.norm1.bias, torch.Size([768])\n",
      "image_encoder.blocks.10.norm1.weight, torch.Size([768])\n",
      "image_encoder.blocks.10.norm2.bias, torch.Size([768])\n",
      "image_encoder.blocks.10.norm2.weight, torch.Size([768])\n",
      "image_encoder.blocks.11.attn.proj.bias, torch.Size([768])\n",
      "image_encoder.blocks.11.attn.proj.weight, torch.Size([768, 768])\n",
      "image_encoder.blocks.11.attn.qkv.bias, torch.Size([2304])\n",
      "image_encoder.blocks.11.attn.qkv.weight, torch.Size([2304, 768])\n",
      "image_encoder.blocks.11.attn.rel_pos_h, torch.Size([127, 64])\n",
      "image_encoder.blocks.11.attn.rel_pos_w, torch.Size([127, 64])\n",
      "image_encoder.blocks.11.mlp.lin1.bias, torch.Size([3072])\n",
      "image_encoder.blocks.11.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "image_encoder.blocks.11.mlp.lin2.bias, torch.Size([768])\n",
      "image_encoder.blocks.11.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "image_encoder.blocks.11.norm1.bias, torch.Size([768])\n",
      "image_encoder.blocks.11.norm1.weight, torch.Size([768])\n",
      "image_encoder.blocks.11.norm2.bias, torch.Size([768])\n",
      "image_encoder.blocks.11.norm2.weight, torch.Size([768])\n",
      "image_encoder.blocks.2.attn.proj.bias, torch.Size([768])\n",
      "image_encoder.blocks.2.attn.proj.weight, torch.Size([768, 768])\n",
      "image_encoder.blocks.2.attn.qkv.bias, torch.Size([2304])\n",
      "image_encoder.blocks.2.attn.qkv.weight, torch.Size([2304, 768])\n",
      "image_encoder.blocks.2.attn.rel_pos_h, torch.Size([127, 64])\n",
      "image_encoder.blocks.2.attn.rel_pos_w, torch.Size([127, 64])\n",
      "image_encoder.blocks.2.mlp.lin1.bias, torch.Size([3072])\n",
      "image_encoder.blocks.2.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "image_encoder.blocks.2.mlp.lin2.bias, torch.Size([768])\n",
      "image_encoder.blocks.2.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "image_encoder.blocks.2.norm1.bias, torch.Size([768])\n",
      "image_encoder.blocks.2.norm1.weight, torch.Size([768])\n",
      "image_encoder.blocks.2.norm2.bias, torch.Size([768])\n",
      "image_encoder.blocks.2.norm2.weight, torch.Size([768])\n",
      "image_encoder.blocks.3.attn.proj.bias, torch.Size([768])\n",
      "image_encoder.blocks.3.attn.proj.weight, torch.Size([768, 768])\n",
      "image_encoder.blocks.3.attn.qkv.bias, torch.Size([2304])\n",
      "image_encoder.blocks.3.attn.qkv.weight, torch.Size([2304, 768])\n",
      "image_encoder.blocks.3.attn.rel_pos_h, torch.Size([27, 64])\n",
      "image_encoder.blocks.3.attn.rel_pos_w, torch.Size([27, 64])\n",
      "image_encoder.blocks.3.mlp.lin1.bias, torch.Size([3072])\n",
      "image_encoder.blocks.3.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "image_encoder.blocks.3.mlp.lin2.bias, torch.Size([768])\n",
      "image_encoder.blocks.3.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "image_encoder.blocks.3.norm1.bias, torch.Size([768])\n",
      "image_encoder.blocks.3.norm1.weight, torch.Size([768])\n",
      "image_encoder.blocks.3.norm2.bias, torch.Size([768])\n",
      "image_encoder.blocks.3.norm2.weight, torch.Size([768])\n",
      "image_encoder.blocks.4.attn.proj.bias, torch.Size([768])\n",
      "image_encoder.blocks.4.attn.proj.weight, torch.Size([768, 768])\n",
      "image_encoder.blocks.4.attn.qkv.bias, torch.Size([2304])\n",
      "image_encoder.blocks.4.attn.qkv.weight, torch.Size([2304, 768])\n",
      "image_encoder.blocks.4.attn.rel_pos_h, torch.Size([27, 64])\n",
      "image_encoder.blocks.4.attn.rel_pos_w, torch.Size([27, 64])\n",
      "image_encoder.blocks.4.mlp.lin1.bias, torch.Size([3072])\n",
      "image_encoder.blocks.4.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "image_encoder.blocks.4.mlp.lin2.bias, torch.Size([768])\n",
      "image_encoder.blocks.4.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "image_encoder.blocks.4.norm1.bias, torch.Size([768])\n",
      "image_encoder.blocks.4.norm1.weight, torch.Size([768])\n",
      "image_encoder.blocks.4.norm2.bias, torch.Size([768])\n",
      "image_encoder.blocks.4.norm2.weight, torch.Size([768])\n",
      "image_encoder.blocks.5.attn.proj.bias, torch.Size([768])\n",
      "image_encoder.blocks.5.attn.proj.weight, torch.Size([768, 768])\n",
      "image_encoder.blocks.5.attn.qkv.bias, torch.Size([2304])\n",
      "image_encoder.blocks.5.attn.qkv.weight, torch.Size([2304, 768])\n",
      "image_encoder.blocks.5.attn.rel_pos_h, torch.Size([127, 64])\n",
      "image_encoder.blocks.5.attn.rel_pos_w, torch.Size([127, 64])\n",
      "image_encoder.blocks.5.mlp.lin1.bias, torch.Size([3072])\n",
      "image_encoder.blocks.5.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "image_encoder.blocks.5.mlp.lin2.bias, torch.Size([768])\n",
      "image_encoder.blocks.5.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "image_encoder.blocks.5.norm1.bias, torch.Size([768])\n",
      "image_encoder.blocks.5.norm1.weight, torch.Size([768])\n",
      "image_encoder.blocks.5.norm2.bias, torch.Size([768])\n",
      "image_encoder.blocks.5.norm2.weight, torch.Size([768])\n",
      "image_encoder.blocks.6.attn.proj.bias, torch.Size([768])\n",
      "image_encoder.blocks.6.attn.proj.weight, torch.Size([768, 768])\n",
      "image_encoder.blocks.6.attn.qkv.bias, torch.Size([2304])\n",
      "image_encoder.blocks.6.attn.qkv.weight, torch.Size([2304, 768])\n",
      "image_encoder.blocks.6.attn.rel_pos_h, torch.Size([27, 64])\n",
      "image_encoder.blocks.6.attn.rel_pos_w, torch.Size([27, 64])\n",
      "image_encoder.blocks.6.mlp.lin1.bias, torch.Size([3072])\n",
      "image_encoder.blocks.6.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "image_encoder.blocks.6.mlp.lin2.bias, torch.Size([768])\n",
      "image_encoder.blocks.6.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "image_encoder.blocks.6.norm1.bias, torch.Size([768])\n",
      "image_encoder.blocks.6.norm1.weight, torch.Size([768])\n",
      "image_encoder.blocks.6.norm2.bias, torch.Size([768])\n",
      "image_encoder.blocks.6.norm2.weight, torch.Size([768])\n",
      "image_encoder.blocks.7.attn.proj.bias, torch.Size([768])\n",
      "image_encoder.blocks.7.attn.proj.weight, torch.Size([768, 768])\n",
      "image_encoder.blocks.7.attn.qkv.bias, torch.Size([2304])\n",
      "image_encoder.blocks.7.attn.qkv.weight, torch.Size([2304, 768])\n",
      "image_encoder.blocks.7.attn.rel_pos_h, torch.Size([27, 64])\n",
      "image_encoder.blocks.7.attn.rel_pos_w, torch.Size([27, 64])\n",
      "image_encoder.blocks.7.mlp.lin1.bias, torch.Size([3072])\n",
      "image_encoder.blocks.7.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "image_encoder.blocks.7.mlp.lin2.bias, torch.Size([768])\n",
      "image_encoder.blocks.7.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "image_encoder.blocks.7.norm1.bias, torch.Size([768])\n",
      "image_encoder.blocks.7.norm1.weight, torch.Size([768])\n",
      "image_encoder.blocks.7.norm2.bias, torch.Size([768])\n",
      "image_encoder.blocks.7.norm2.weight, torch.Size([768])\n",
      "image_encoder.blocks.8.attn.proj.bias, torch.Size([768])\n",
      "image_encoder.blocks.8.attn.proj.weight, torch.Size([768, 768])\n",
      "image_encoder.blocks.8.attn.qkv.bias, torch.Size([2304])\n",
      "image_encoder.blocks.8.attn.qkv.weight, torch.Size([2304, 768])\n",
      "image_encoder.blocks.8.attn.rel_pos_h, torch.Size([127, 64])\n",
      "image_encoder.blocks.8.attn.rel_pos_w, torch.Size([127, 64])\n",
      "image_encoder.blocks.8.mlp.lin1.bias, torch.Size([3072])\n",
      "image_encoder.blocks.8.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "image_encoder.blocks.8.mlp.lin2.bias, torch.Size([768])\n",
      "image_encoder.blocks.8.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "image_encoder.blocks.8.norm1.bias, torch.Size([768])\n",
      "image_encoder.blocks.8.norm1.weight, torch.Size([768])\n",
      "image_encoder.blocks.8.norm2.bias, torch.Size([768])\n",
      "image_encoder.blocks.8.norm2.weight, torch.Size([768])\n",
      "image_encoder.blocks.9.attn.proj.bias, torch.Size([768])\n",
      "image_encoder.blocks.9.attn.proj.weight, torch.Size([768, 768])\n",
      "image_encoder.blocks.9.attn.qkv.bias, torch.Size([2304])\n",
      "image_encoder.blocks.9.attn.qkv.weight, torch.Size([2304, 768])\n",
      "image_encoder.blocks.9.attn.rel_pos_h, torch.Size([27, 64])\n",
      "image_encoder.blocks.9.attn.rel_pos_w, torch.Size([27, 64])\n",
      "image_encoder.blocks.9.mlp.lin1.bias, torch.Size([3072])\n",
      "image_encoder.blocks.9.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "image_encoder.blocks.9.mlp.lin2.bias, torch.Size([768])\n",
      "image_encoder.blocks.9.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "image_encoder.blocks.9.norm1.bias, torch.Size([768])\n",
      "image_encoder.blocks.9.norm1.weight, torch.Size([768])\n",
      "image_encoder.blocks.9.norm2.bias, torch.Size([768])\n",
      "image_encoder.blocks.9.norm2.weight, torch.Size([768])\n",
      "image_encoder.neck.0.weight, torch.Size([256, 768, 1, 1])\n",
      "image_encoder.neck.1.bias, torch.Size([256])\n",
      "image_encoder.neck.1.weight, torch.Size([256])\n",
      "image_encoder.neck.2.weight, torch.Size([256, 256, 3, 3])\n",
      "image_encoder.neck.3.bias, torch.Size([256])\n",
      "image_encoder.neck.3.weight, torch.Size([256])\n",
      "image_encoder.patch_embed.proj.bias, torch.Size([768])\n",
      "image_encoder.patch_embed.proj.weight, torch.Size([768, 3, 16, 16])\n",
      "image_encoder.pos_embed, torch.Size([1, 64, 64, 768])\n",
      "mask_decoder.iou_prediction_head.layers.0.bias, torch.Size([256])\n",
      "mask_decoder.iou_prediction_head.layers.0.weight, torch.Size([256, 256])\n",
      "mask_decoder.iou_prediction_head.layers.1.bias, torch.Size([256])\n",
      "mask_decoder.iou_prediction_head.layers.1.weight, torch.Size([256, 256])\n",
      "mask_decoder.iou_prediction_head.layers.2.bias, torch.Size([4])\n",
      "mask_decoder.iou_prediction_head.layers.2.weight, torch.Size([4, 256])\n",
      "mask_decoder.iou_token.weight, torch.Size([1, 256])\n",
      "mask_decoder.mask_tokens.weight, torch.Size([4, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.0.bias, torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.0.weight, torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.1.bias, torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.1.weight, torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.2.bias, torch.Size([32])\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.2.weight, torch.Size([32, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.0.bias, torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.0.weight, torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.1.bias, torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.1.weight, torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.2.bias, torch.Size([32])\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.2.weight, torch.Size([32, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.0.bias, torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.0.weight, torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.1.bias, torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.1.weight, torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.2.bias, torch.Size([32])\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.2.weight, torch.Size([32, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.0.bias, torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.0.weight, torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.1.bias, torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.1.weight, torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.2.bias, torch.Size([32])\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.2.weight, torch.Size([32, 256])\n",
      "mask_decoder.output_upscaling.0.bias, torch.Size([64])\n",
      "mask_decoder.output_upscaling.0.weight, torch.Size([256, 64, 2, 2])\n",
      "mask_decoder.output_upscaling.1.bias, torch.Size([64])\n",
      "mask_decoder.output_upscaling.1.weight, torch.Size([64])\n",
      "mask_decoder.output_upscaling.3.bias, torch.Size([32])\n",
      "mask_decoder.output_upscaling.3.weight, torch.Size([64, 32, 2, 2])\n",
      "mask_decoder.transformer.final_attn_token_to_image.k_proj.bias, torch.Size([128])\n",
      "mask_decoder.transformer.final_attn_token_to_image.k_proj.weight, torch.Size([128, 256])\n",
      "mask_decoder.transformer.final_attn_token_to_image.out_proj.bias, torch.Size([256])\n",
      "mask_decoder.transformer.final_attn_token_to_image.out_proj.weight, torch.Size([256, 128])\n",
      "mask_decoder.transformer.final_attn_token_to_image.q_proj.bias, torch.Size([128])\n",
      "mask_decoder.transformer.final_attn_token_to_image.q_proj.weight, torch.Size([128, 256])\n",
      "mask_decoder.transformer.final_attn_token_to_image.v_proj.bias, torch.Size([128])\n",
      "mask_decoder.transformer.final_attn_token_to_image.v_proj.weight, torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias, torch.Size([128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight, torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight, torch.Size([256, 128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias, torch.Size([128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight, torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias, torch.Size([128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight, torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias, torch.Size([128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight, torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight, torch.Size([256, 128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias, torch.Size([128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight, torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias, torch.Size([128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight, torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.0.mlp.lin1.bias, torch.Size([2048])\n",
      "mask_decoder.transformer.layers.0.mlp.lin1.weight, torch.Size([2048, 256])\n",
      "mask_decoder.transformer.layers.0.mlp.lin2.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.mlp.lin2.weight, torch.Size([256, 2048])\n",
      "mask_decoder.transformer.layers.0.norm1.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.norm1.weight, torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.norm2.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.norm2.weight, torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.norm3.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.norm3.weight, torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.norm4.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.norm4.weight, torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.self_attn.k_proj.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.self_attn.k_proj.weight, torch.Size([256, 256])\n",
      "mask_decoder.transformer.layers.0.self_attn.out_proj.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.self_attn.out_proj.weight, torch.Size([256, 256])\n",
      "mask_decoder.transformer.layers.0.self_attn.q_proj.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.self_attn.q_proj.weight, torch.Size([256, 256])\n",
      "mask_decoder.transformer.layers.0.self_attn.v_proj.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.self_attn.v_proj.weight, torch.Size([256, 256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias, torch.Size([128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight, torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight, torch.Size([256, 128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias, torch.Size([128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight, torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias, torch.Size([128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight, torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias, torch.Size([128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight, torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight, torch.Size([256, 128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias, torch.Size([128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight, torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias, torch.Size([128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight, torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.1.mlp.lin1.bias, torch.Size([2048])\n",
      "mask_decoder.transformer.layers.1.mlp.lin1.weight, torch.Size([2048, 256])\n",
      "mask_decoder.transformer.layers.1.mlp.lin2.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.mlp.lin2.weight, torch.Size([256, 2048])\n",
      "mask_decoder.transformer.layers.1.norm1.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.norm1.weight, torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.norm2.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.norm2.weight, torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.norm3.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.norm3.weight, torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.norm4.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.norm4.weight, torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.self_attn.k_proj.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.self_attn.k_proj.weight, torch.Size([256, 256])\n",
      "mask_decoder.transformer.layers.1.self_attn.out_proj.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.self_attn.out_proj.weight, torch.Size([256, 256])\n",
      "mask_decoder.transformer.layers.1.self_attn.q_proj.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.self_attn.q_proj.weight, torch.Size([256, 256])\n",
      "mask_decoder.transformer.layers.1.self_attn.v_proj.bias, torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.self_attn.v_proj.weight, torch.Size([256, 256])\n",
      "mask_decoder.transformer.norm_final_attn.bias, torch.Size([256])\n",
      "mask_decoder.transformer.norm_final_attn.weight, torch.Size([256])\n",
      "prompt_encoder.mask_downscaling.0.bias, torch.Size([4])\n",
      "prompt_encoder.mask_downscaling.0.weight, torch.Size([4, 1, 2, 2])\n",
      "prompt_encoder.mask_downscaling.1.bias, torch.Size([4])\n",
      "prompt_encoder.mask_downscaling.1.weight, torch.Size([4])\n",
      "prompt_encoder.mask_downscaling.3.bias, torch.Size([16])\n",
      "prompt_encoder.mask_downscaling.3.weight, torch.Size([16, 4, 2, 2])\n",
      "prompt_encoder.mask_downscaling.4.bias, torch.Size([16])\n",
      "prompt_encoder.mask_downscaling.4.weight, torch.Size([16])\n",
      "prompt_encoder.mask_downscaling.6.bias, torch.Size([256])\n",
      "prompt_encoder.mask_downscaling.6.weight, torch.Size([256, 16, 1, 1])\n",
      "prompt_encoder.no_mask_embed.weight, torch.Size([1, 256])\n",
      "prompt_encoder.not_a_point_embed.weight, torch.Size([1, 256])\n",
      "prompt_encoder.pe_layer.positional_encoding_gaussian_matrix, torch.Size([2, 128])\n",
      "prompt_encoder.point_embeddings.0.weight, torch.Size([1, 256])\n",
      "prompt_encoder.point_embeddings.1.weight, torch.Size([1, 256])\n",
      "prompt_encoder.point_embeddings.2.weight, torch.Size([1, 256])\n",
      "prompt_encoder.point_embeddings.3.weight, torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "for k in sorted(state_dict_sam_pth.keys()):\n",
    "    print(f\"{k}, {state_dict_sam_pth[k].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e148c0-f4b3-4141-803a-6ddc483fc246",
   "metadata": {},
   "source": [
    "# 二 模型权重转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56cdd0a6-e2f3-472a-b372-6d9a9a849e0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T01:22:19.949135Z",
     "iopub.status.busy": "2025-11-06T01:22:19.948876Z",
     "iopub.status.idle": "2025-11-06T01:22:20.723548Z",
     "shell.execute_reply": "2025-11-06T01:22:20.723096Z",
     "shell.execute_reply.started": "2025-11-06T01:22:19.949116Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix_len = len('image_encoder.')\n",
    "\n",
    "state_dict_sam_pth_processed = {}\n",
    "\n",
    "for k, v in state_dict_sam_pth.items():\n",
    "    if k.startswith('image_encoder.'):\n",
    "        new_key = k[prefix_len:]\n",
    "        state_dict_sam_pth_processed[new_key] = v\n",
    "\n",
    "checkpoint_sam_pth_processed = '/mnt/workspace/models/facebook/sam-vit-base/sam_vit_b_01ec64_processed.pth'\n",
    "torch.save(state_dict_sam_pth_processed, checkpoint_sam_pth_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87cb1d6f-e47e-407a-b451-52ba1d6ed8f3",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-06T01:22:50.594857Z",
     "iopub.status.busy": "2025-11-06T01:22:50.594607Z",
     "iopub.status.idle": "2025-11-06T01:22:50.599845Z",
     "shell.execute_reply": "2025-11-06T01:22:50.599428Z",
     "shell.execute_reply.started": "2025-11-06T01:22:50.594839Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.attn.proj.bias, torch.Size([768])\n",
      "blocks.0.attn.proj.weight, torch.Size([768, 768])\n",
      "blocks.0.attn.qkv.bias, torch.Size([2304])\n",
      "blocks.0.attn.qkv.weight, torch.Size([2304, 768])\n",
      "blocks.0.attn.rel_pos_h, torch.Size([27, 64])\n",
      "blocks.0.attn.rel_pos_w, torch.Size([27, 64])\n",
      "blocks.0.mlp.lin1.bias, torch.Size([3072])\n",
      "blocks.0.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "blocks.0.mlp.lin2.bias, torch.Size([768])\n",
      "blocks.0.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "blocks.0.norm1.bias, torch.Size([768])\n",
      "blocks.0.norm1.weight, torch.Size([768])\n",
      "blocks.0.norm2.bias, torch.Size([768])\n",
      "blocks.0.norm2.weight, torch.Size([768])\n",
      "blocks.1.attn.proj.bias, torch.Size([768])\n",
      "blocks.1.attn.proj.weight, torch.Size([768, 768])\n",
      "blocks.1.attn.qkv.bias, torch.Size([2304])\n",
      "blocks.1.attn.qkv.weight, torch.Size([2304, 768])\n",
      "blocks.1.attn.rel_pos_h, torch.Size([27, 64])\n",
      "blocks.1.attn.rel_pos_w, torch.Size([27, 64])\n",
      "blocks.1.mlp.lin1.bias, torch.Size([3072])\n",
      "blocks.1.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "blocks.1.mlp.lin2.bias, torch.Size([768])\n",
      "blocks.1.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "blocks.1.norm1.bias, torch.Size([768])\n",
      "blocks.1.norm1.weight, torch.Size([768])\n",
      "blocks.1.norm2.bias, torch.Size([768])\n",
      "blocks.1.norm2.weight, torch.Size([768])\n",
      "blocks.10.attn.proj.bias, torch.Size([768])\n",
      "blocks.10.attn.proj.weight, torch.Size([768, 768])\n",
      "blocks.10.attn.qkv.bias, torch.Size([2304])\n",
      "blocks.10.attn.qkv.weight, torch.Size([2304, 768])\n",
      "blocks.10.attn.rel_pos_h, torch.Size([27, 64])\n",
      "blocks.10.attn.rel_pos_w, torch.Size([27, 64])\n",
      "blocks.10.mlp.lin1.bias, torch.Size([3072])\n",
      "blocks.10.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "blocks.10.mlp.lin2.bias, torch.Size([768])\n",
      "blocks.10.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "blocks.10.norm1.bias, torch.Size([768])\n",
      "blocks.10.norm1.weight, torch.Size([768])\n",
      "blocks.10.norm2.bias, torch.Size([768])\n",
      "blocks.10.norm2.weight, torch.Size([768])\n",
      "blocks.11.attn.proj.bias, torch.Size([768])\n",
      "blocks.11.attn.proj.weight, torch.Size([768, 768])\n",
      "blocks.11.attn.qkv.bias, torch.Size([2304])\n",
      "blocks.11.attn.qkv.weight, torch.Size([2304, 768])\n",
      "blocks.11.attn.rel_pos_h, torch.Size([127, 64])\n",
      "blocks.11.attn.rel_pos_w, torch.Size([127, 64])\n",
      "blocks.11.mlp.lin1.bias, torch.Size([3072])\n",
      "blocks.11.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "blocks.11.mlp.lin2.bias, torch.Size([768])\n",
      "blocks.11.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "blocks.11.norm1.bias, torch.Size([768])\n",
      "blocks.11.norm1.weight, torch.Size([768])\n",
      "blocks.11.norm2.bias, torch.Size([768])\n",
      "blocks.11.norm2.weight, torch.Size([768])\n",
      "blocks.2.attn.proj.bias, torch.Size([768])\n",
      "blocks.2.attn.proj.weight, torch.Size([768, 768])\n",
      "blocks.2.attn.qkv.bias, torch.Size([2304])\n",
      "blocks.2.attn.qkv.weight, torch.Size([2304, 768])\n",
      "blocks.2.attn.rel_pos_h, torch.Size([127, 64])\n",
      "blocks.2.attn.rel_pos_w, torch.Size([127, 64])\n",
      "blocks.2.mlp.lin1.bias, torch.Size([3072])\n",
      "blocks.2.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "blocks.2.mlp.lin2.bias, torch.Size([768])\n",
      "blocks.2.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "blocks.2.norm1.bias, torch.Size([768])\n",
      "blocks.2.norm1.weight, torch.Size([768])\n",
      "blocks.2.norm2.bias, torch.Size([768])\n",
      "blocks.2.norm2.weight, torch.Size([768])\n",
      "blocks.3.attn.proj.bias, torch.Size([768])\n",
      "blocks.3.attn.proj.weight, torch.Size([768, 768])\n",
      "blocks.3.attn.qkv.bias, torch.Size([2304])\n",
      "blocks.3.attn.qkv.weight, torch.Size([2304, 768])\n",
      "blocks.3.attn.rel_pos_h, torch.Size([27, 64])\n",
      "blocks.3.attn.rel_pos_w, torch.Size([27, 64])\n",
      "blocks.3.mlp.lin1.bias, torch.Size([3072])\n",
      "blocks.3.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "blocks.3.mlp.lin2.bias, torch.Size([768])\n",
      "blocks.3.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "blocks.3.norm1.bias, torch.Size([768])\n",
      "blocks.3.norm1.weight, torch.Size([768])\n",
      "blocks.3.norm2.bias, torch.Size([768])\n",
      "blocks.3.norm2.weight, torch.Size([768])\n",
      "blocks.4.attn.proj.bias, torch.Size([768])\n",
      "blocks.4.attn.proj.weight, torch.Size([768, 768])\n",
      "blocks.4.attn.qkv.bias, torch.Size([2304])\n",
      "blocks.4.attn.qkv.weight, torch.Size([2304, 768])\n",
      "blocks.4.attn.rel_pos_h, torch.Size([27, 64])\n",
      "blocks.4.attn.rel_pos_w, torch.Size([27, 64])\n",
      "blocks.4.mlp.lin1.bias, torch.Size([3072])\n",
      "blocks.4.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "blocks.4.mlp.lin2.bias, torch.Size([768])\n",
      "blocks.4.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "blocks.4.norm1.bias, torch.Size([768])\n",
      "blocks.4.norm1.weight, torch.Size([768])\n",
      "blocks.4.norm2.bias, torch.Size([768])\n",
      "blocks.4.norm2.weight, torch.Size([768])\n",
      "blocks.5.attn.proj.bias, torch.Size([768])\n",
      "blocks.5.attn.proj.weight, torch.Size([768, 768])\n",
      "blocks.5.attn.qkv.bias, torch.Size([2304])\n",
      "blocks.5.attn.qkv.weight, torch.Size([2304, 768])\n",
      "blocks.5.attn.rel_pos_h, torch.Size([127, 64])\n",
      "blocks.5.attn.rel_pos_w, torch.Size([127, 64])\n",
      "blocks.5.mlp.lin1.bias, torch.Size([3072])\n",
      "blocks.5.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "blocks.5.mlp.lin2.bias, torch.Size([768])\n",
      "blocks.5.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "blocks.5.norm1.bias, torch.Size([768])\n",
      "blocks.5.norm1.weight, torch.Size([768])\n",
      "blocks.5.norm2.bias, torch.Size([768])\n",
      "blocks.5.norm2.weight, torch.Size([768])\n",
      "blocks.6.attn.proj.bias, torch.Size([768])\n",
      "blocks.6.attn.proj.weight, torch.Size([768, 768])\n",
      "blocks.6.attn.qkv.bias, torch.Size([2304])\n",
      "blocks.6.attn.qkv.weight, torch.Size([2304, 768])\n",
      "blocks.6.attn.rel_pos_h, torch.Size([27, 64])\n",
      "blocks.6.attn.rel_pos_w, torch.Size([27, 64])\n",
      "blocks.6.mlp.lin1.bias, torch.Size([3072])\n",
      "blocks.6.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "blocks.6.mlp.lin2.bias, torch.Size([768])\n",
      "blocks.6.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "blocks.6.norm1.bias, torch.Size([768])\n",
      "blocks.6.norm1.weight, torch.Size([768])\n",
      "blocks.6.norm2.bias, torch.Size([768])\n",
      "blocks.6.norm2.weight, torch.Size([768])\n",
      "blocks.7.attn.proj.bias, torch.Size([768])\n",
      "blocks.7.attn.proj.weight, torch.Size([768, 768])\n",
      "blocks.7.attn.qkv.bias, torch.Size([2304])\n",
      "blocks.7.attn.qkv.weight, torch.Size([2304, 768])\n",
      "blocks.7.attn.rel_pos_h, torch.Size([27, 64])\n",
      "blocks.7.attn.rel_pos_w, torch.Size([27, 64])\n",
      "blocks.7.mlp.lin1.bias, torch.Size([3072])\n",
      "blocks.7.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "blocks.7.mlp.lin2.bias, torch.Size([768])\n",
      "blocks.7.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "blocks.7.norm1.bias, torch.Size([768])\n",
      "blocks.7.norm1.weight, torch.Size([768])\n",
      "blocks.7.norm2.bias, torch.Size([768])\n",
      "blocks.7.norm2.weight, torch.Size([768])\n",
      "blocks.8.attn.proj.bias, torch.Size([768])\n",
      "blocks.8.attn.proj.weight, torch.Size([768, 768])\n",
      "blocks.8.attn.qkv.bias, torch.Size([2304])\n",
      "blocks.8.attn.qkv.weight, torch.Size([2304, 768])\n",
      "blocks.8.attn.rel_pos_h, torch.Size([127, 64])\n",
      "blocks.8.attn.rel_pos_w, torch.Size([127, 64])\n",
      "blocks.8.mlp.lin1.bias, torch.Size([3072])\n",
      "blocks.8.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "blocks.8.mlp.lin2.bias, torch.Size([768])\n",
      "blocks.8.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "blocks.8.norm1.bias, torch.Size([768])\n",
      "blocks.8.norm1.weight, torch.Size([768])\n",
      "blocks.8.norm2.bias, torch.Size([768])\n",
      "blocks.8.norm2.weight, torch.Size([768])\n",
      "blocks.9.attn.proj.bias, torch.Size([768])\n",
      "blocks.9.attn.proj.weight, torch.Size([768, 768])\n",
      "blocks.9.attn.qkv.bias, torch.Size([2304])\n",
      "blocks.9.attn.qkv.weight, torch.Size([2304, 768])\n",
      "blocks.9.attn.rel_pos_h, torch.Size([27, 64])\n",
      "blocks.9.attn.rel_pos_w, torch.Size([27, 64])\n",
      "blocks.9.mlp.lin1.bias, torch.Size([3072])\n",
      "blocks.9.mlp.lin1.weight, torch.Size([3072, 768])\n",
      "blocks.9.mlp.lin2.bias, torch.Size([768])\n",
      "blocks.9.mlp.lin2.weight, torch.Size([768, 3072])\n",
      "blocks.9.norm1.bias, torch.Size([768])\n",
      "blocks.9.norm1.weight, torch.Size([768])\n",
      "blocks.9.norm2.bias, torch.Size([768])\n",
      "blocks.9.norm2.weight, torch.Size([768])\n",
      "neck.0.weight, torch.Size([256, 768, 1, 1])\n",
      "neck.1.bias, torch.Size([256])\n",
      "neck.1.weight, torch.Size([256])\n",
      "neck.2.weight, torch.Size([256, 256, 3, 3])\n",
      "neck.3.bias, torch.Size([256])\n",
      "neck.3.weight, torch.Size([256])\n",
      "patch_embed.proj.bias, torch.Size([768])\n",
      "patch_embed.proj.weight, torch.Size([768, 3, 16, 16])\n",
      "pos_embed, torch.Size([1, 64, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "for k in sorted(state_dict_sam_pth_processed.keys()):\n",
    "    print(f\"{k}, {state_dict_sam_pth_processed[k].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171f737-fa2f-4797-b724-bd4560783cf3",
   "metadata": {},
   "source": [
    "# 三 转换结果比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a4dfb2e-b5d7-417b-a06a-24bb6f8e694a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T01:30:30.450475Z",
     "iopub.status.busy": "2025-11-06T01:30:30.450266Z",
     "iopub.status.idle": "2025-11-06T01:30:30.454408Z",
     "shell.execute_reply": "2025-11-06T01:30:30.454030Z",
     "shell.execute_reply.started": "2025-11-06T01:30:30.450460Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0038,  0.0193,  0.0006,  ...,  0.0039, -0.0237, -0.0004],\n",
       "        [-0.0127,  0.0071, -0.0154,  ..., -0.0171, -0.0293, -0.0257],\n",
       "        [ 0.0105, -0.0062, -0.0084,  ..., -0.0066,  0.0157, -0.0397],\n",
       "        ...,\n",
       "        [-0.0033, -0.0086,  0.0045,  ..., -0.0479, -0.0259, -0.0090],\n",
       "        [ 0.0052, -0.0122, -0.0056,  ...,  0.0063,  0.0052,  0.0098],\n",
       "        [ 0.0094, -0.0138, -0.0194,  ..., -0.0167,  0.0060, -0.0521]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_sam_pth['image_encoder.blocks.0.attn.proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b428c617-7aff-4f74-8c16-79b7651e77b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T01:30:38.307543Z",
     "iopub.status.busy": "2025-11-06T01:30:38.307326Z",
     "iopub.status.idle": "2025-11-06T01:30:38.311438Z",
     "shell.execute_reply": "2025-11-06T01:30:38.311058Z",
     "shell.execute_reply.started": "2025-11-06T01:30:38.307528Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0038,  0.0193,  0.0006,  ...,  0.0039, -0.0237, -0.0004],\n",
       "        [-0.0127,  0.0071, -0.0154,  ..., -0.0171, -0.0293, -0.0257],\n",
       "        [ 0.0105, -0.0062, -0.0084,  ..., -0.0066,  0.0157, -0.0397],\n",
       "        ...,\n",
       "        [-0.0033, -0.0086,  0.0045,  ..., -0.0479, -0.0259, -0.0090],\n",
       "        [ 0.0052, -0.0122, -0.0056,  ...,  0.0063,  0.0052,  0.0098],\n",
       "        [ 0.0094, -0.0138, -0.0194,  ..., -0.0167,  0.0060, -0.0521]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_sam_pth_processed['blocks.0.attn.proj.weight']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
