{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4e4bf1-e4fe-4ad7-bdab-52d4a98dce0f",
   "metadata": {},
   "source": [
    "# 一 模型权重分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b49b26d9-2072-4056-bf8a-f542392d72af",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-11-06T01:59:34.722410Z",
     "iopub.status.busy": "2025-11-06T01:59:34.722203Z",
     "iopub.status.idle": "2025-11-06T01:59:34.724603Z",
     "shell.execute_reply": "2025-11-06T01:59:34.724224Z",
     "shell.execute_reply.started": "2025-11-06T01:59:34.722394Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f78b57ba-0c37-40a7-8607-346bf7605afd",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-06T01:59:35.779680Z",
     "iopub.status.busy": "2025-11-06T01:59:35.779457Z",
     "iopub.status.idle": "2025-11-06T01:59:35.781927Z",
     "shell.execute_reply": "2025-11-06T01:59:35.781530Z",
     "shell.execute_reply.started": "2025-11-06T01:59:35.779647Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_clip_bin = '/mnt/workspace/models/openai/clip-vit-large-patch14/pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3857519-c15a-4470-bbb3-bff13f10ee44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T01:59:36.943047Z",
     "iopub.status.busy": "2025-11-06T01:59:36.942830Z",
     "iopub.status.idle": "2025-11-06T01:59:47.399197Z",
     "shell.execute_reply": "2025-11-06T01:59:47.398723Z",
     "shell.execute_reply.started": "2025-11-06T01:59:36.943029Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_dict_clip_bin = torch.load(checkpoint_clip_bin, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f0b138e-6903-4c03-a553-816a515b4d79",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-06T02:13:51.860809Z",
     "iopub.status.busy": "2025-11-06T02:13:51.860600Z",
     "iopub.status.idle": "2025-11-06T02:13:51.868648Z",
     "shell.execute_reply": "2025-11-06T02:13:51.868252Z",
     "shell.execute_reply.started": "2025-11-06T02:13:51.860793Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_scale, torch.Size([])\n",
      "text_model.embeddings.position_embedding.weight, torch.Size([77, 768])\n",
      "text_model.embeddings.position_ids, torch.Size([1, 77])\n",
      "text_model.embeddings.token_embedding.weight, torch.Size([49408, 768])\n",
      "text_model.encoder.layers.0.layer_norm1.bias, torch.Size([768])\n",
      "text_model.encoder.layers.0.layer_norm1.weight, torch.Size([768])\n",
      "text_model.encoder.layers.0.layer_norm2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.0.layer_norm2.weight, torch.Size([768])\n",
      "text_model.encoder.layers.0.mlp.fc1.bias, torch.Size([3072])\n",
      "text_model.encoder.layers.0.mlp.fc1.weight, torch.Size([3072, 768])\n",
      "text_model.encoder.layers.0.mlp.fc2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.0.mlp.fc2.weight, torch.Size([768, 3072])\n",
      "text_model.encoder.layers.0.self_attn.k_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.0.self_attn.k_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.0.self_attn.out_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.0.self_attn.out_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.0.self_attn.q_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.0.self_attn.q_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.0.self_attn.v_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.0.self_attn.v_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.1.layer_norm1.bias, torch.Size([768])\n",
      "text_model.encoder.layers.1.layer_norm1.weight, torch.Size([768])\n",
      "text_model.encoder.layers.1.layer_norm2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.1.layer_norm2.weight, torch.Size([768])\n",
      "text_model.encoder.layers.1.mlp.fc1.bias, torch.Size([3072])\n",
      "text_model.encoder.layers.1.mlp.fc1.weight, torch.Size([3072, 768])\n",
      "text_model.encoder.layers.1.mlp.fc2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.1.mlp.fc2.weight, torch.Size([768, 3072])\n",
      "text_model.encoder.layers.1.self_attn.k_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.1.self_attn.k_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.1.self_attn.out_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.1.self_attn.out_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.1.self_attn.q_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.1.self_attn.q_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.1.self_attn.v_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.1.self_attn.v_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.10.layer_norm1.bias, torch.Size([768])\n",
      "text_model.encoder.layers.10.layer_norm1.weight, torch.Size([768])\n",
      "text_model.encoder.layers.10.layer_norm2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.10.layer_norm2.weight, torch.Size([768])\n",
      "text_model.encoder.layers.10.mlp.fc1.bias, torch.Size([3072])\n",
      "text_model.encoder.layers.10.mlp.fc1.weight, torch.Size([3072, 768])\n",
      "text_model.encoder.layers.10.mlp.fc2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.10.mlp.fc2.weight, torch.Size([768, 3072])\n",
      "text_model.encoder.layers.10.self_attn.k_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.10.self_attn.k_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.10.self_attn.out_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.10.self_attn.out_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.10.self_attn.q_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.10.self_attn.q_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.10.self_attn.v_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.10.self_attn.v_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.11.layer_norm1.bias, torch.Size([768])\n",
      "text_model.encoder.layers.11.layer_norm1.weight, torch.Size([768])\n",
      "text_model.encoder.layers.11.layer_norm2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.11.layer_norm2.weight, torch.Size([768])\n",
      "text_model.encoder.layers.11.mlp.fc1.bias, torch.Size([3072])\n",
      "text_model.encoder.layers.11.mlp.fc1.weight, torch.Size([3072, 768])\n",
      "text_model.encoder.layers.11.mlp.fc2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.11.mlp.fc2.weight, torch.Size([768, 3072])\n",
      "text_model.encoder.layers.11.self_attn.k_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.11.self_attn.k_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.11.self_attn.out_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.11.self_attn.out_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.11.self_attn.q_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.11.self_attn.q_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.11.self_attn.v_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.11.self_attn.v_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.2.layer_norm1.bias, torch.Size([768])\n",
      "text_model.encoder.layers.2.layer_norm1.weight, torch.Size([768])\n",
      "text_model.encoder.layers.2.layer_norm2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.2.layer_norm2.weight, torch.Size([768])\n",
      "text_model.encoder.layers.2.mlp.fc1.bias, torch.Size([3072])\n",
      "text_model.encoder.layers.2.mlp.fc1.weight, torch.Size([3072, 768])\n",
      "text_model.encoder.layers.2.mlp.fc2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.2.mlp.fc2.weight, torch.Size([768, 3072])\n",
      "text_model.encoder.layers.2.self_attn.k_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.2.self_attn.k_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.2.self_attn.out_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.2.self_attn.out_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.2.self_attn.q_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.2.self_attn.q_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.2.self_attn.v_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.2.self_attn.v_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.3.layer_norm1.bias, torch.Size([768])\n",
      "text_model.encoder.layers.3.layer_norm1.weight, torch.Size([768])\n",
      "text_model.encoder.layers.3.layer_norm2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.3.layer_norm2.weight, torch.Size([768])\n",
      "text_model.encoder.layers.3.mlp.fc1.bias, torch.Size([3072])\n",
      "text_model.encoder.layers.3.mlp.fc1.weight, torch.Size([3072, 768])\n",
      "text_model.encoder.layers.3.mlp.fc2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.3.mlp.fc2.weight, torch.Size([768, 3072])\n",
      "text_model.encoder.layers.3.self_attn.k_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.3.self_attn.k_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.3.self_attn.out_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.3.self_attn.out_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.3.self_attn.q_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.3.self_attn.q_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.3.self_attn.v_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.3.self_attn.v_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.4.layer_norm1.bias, torch.Size([768])\n",
      "text_model.encoder.layers.4.layer_norm1.weight, torch.Size([768])\n",
      "text_model.encoder.layers.4.layer_norm2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.4.layer_norm2.weight, torch.Size([768])\n",
      "text_model.encoder.layers.4.mlp.fc1.bias, torch.Size([3072])\n",
      "text_model.encoder.layers.4.mlp.fc1.weight, torch.Size([3072, 768])\n",
      "text_model.encoder.layers.4.mlp.fc2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.4.mlp.fc2.weight, torch.Size([768, 3072])\n",
      "text_model.encoder.layers.4.self_attn.k_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.4.self_attn.k_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.4.self_attn.out_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.4.self_attn.out_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.4.self_attn.q_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.4.self_attn.q_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.4.self_attn.v_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.4.self_attn.v_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.5.layer_norm1.bias, torch.Size([768])\n",
      "text_model.encoder.layers.5.layer_norm1.weight, torch.Size([768])\n",
      "text_model.encoder.layers.5.layer_norm2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.5.layer_norm2.weight, torch.Size([768])\n",
      "text_model.encoder.layers.5.mlp.fc1.bias, torch.Size([3072])\n",
      "text_model.encoder.layers.5.mlp.fc1.weight, torch.Size([3072, 768])\n",
      "text_model.encoder.layers.5.mlp.fc2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.5.mlp.fc2.weight, torch.Size([768, 3072])\n",
      "text_model.encoder.layers.5.self_attn.k_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.5.self_attn.k_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.5.self_attn.out_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.5.self_attn.out_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.5.self_attn.q_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.5.self_attn.q_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.5.self_attn.v_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.5.self_attn.v_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.6.layer_norm1.bias, torch.Size([768])\n",
      "text_model.encoder.layers.6.layer_norm1.weight, torch.Size([768])\n",
      "text_model.encoder.layers.6.layer_norm2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.6.layer_norm2.weight, torch.Size([768])\n",
      "text_model.encoder.layers.6.mlp.fc1.bias, torch.Size([3072])\n",
      "text_model.encoder.layers.6.mlp.fc1.weight, torch.Size([3072, 768])\n",
      "text_model.encoder.layers.6.mlp.fc2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.6.mlp.fc2.weight, torch.Size([768, 3072])\n",
      "text_model.encoder.layers.6.self_attn.k_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.6.self_attn.k_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.6.self_attn.out_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.6.self_attn.out_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.6.self_attn.q_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.6.self_attn.q_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.6.self_attn.v_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.6.self_attn.v_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.7.layer_norm1.bias, torch.Size([768])\n",
      "text_model.encoder.layers.7.layer_norm1.weight, torch.Size([768])\n",
      "text_model.encoder.layers.7.layer_norm2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.7.layer_norm2.weight, torch.Size([768])\n",
      "text_model.encoder.layers.7.mlp.fc1.bias, torch.Size([3072])\n",
      "text_model.encoder.layers.7.mlp.fc1.weight, torch.Size([3072, 768])\n",
      "text_model.encoder.layers.7.mlp.fc2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.7.mlp.fc2.weight, torch.Size([768, 3072])\n",
      "text_model.encoder.layers.7.self_attn.k_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.7.self_attn.k_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.7.self_attn.out_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.7.self_attn.out_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.7.self_attn.q_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.7.self_attn.q_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.7.self_attn.v_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.7.self_attn.v_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.8.layer_norm1.bias, torch.Size([768])\n",
      "text_model.encoder.layers.8.layer_norm1.weight, torch.Size([768])\n",
      "text_model.encoder.layers.8.layer_norm2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.8.layer_norm2.weight, torch.Size([768])\n",
      "text_model.encoder.layers.8.mlp.fc1.bias, torch.Size([3072])\n",
      "text_model.encoder.layers.8.mlp.fc1.weight, torch.Size([3072, 768])\n",
      "text_model.encoder.layers.8.mlp.fc2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.8.mlp.fc2.weight, torch.Size([768, 3072])\n",
      "text_model.encoder.layers.8.self_attn.k_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.8.self_attn.k_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.8.self_attn.out_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.8.self_attn.out_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.8.self_attn.q_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.8.self_attn.q_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.8.self_attn.v_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.8.self_attn.v_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.9.layer_norm1.bias, torch.Size([768])\n",
      "text_model.encoder.layers.9.layer_norm1.weight, torch.Size([768])\n",
      "text_model.encoder.layers.9.layer_norm2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.9.layer_norm2.weight, torch.Size([768])\n",
      "text_model.encoder.layers.9.mlp.fc1.bias, torch.Size([3072])\n",
      "text_model.encoder.layers.9.mlp.fc1.weight, torch.Size([3072, 768])\n",
      "text_model.encoder.layers.9.mlp.fc2.bias, torch.Size([768])\n",
      "text_model.encoder.layers.9.mlp.fc2.weight, torch.Size([768, 3072])\n",
      "text_model.encoder.layers.9.self_attn.k_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.9.self_attn.k_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.9.self_attn.out_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.9.self_attn.out_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.9.self_attn.q_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.9.self_attn.q_proj.weight, torch.Size([768, 768])\n",
      "text_model.encoder.layers.9.self_attn.v_proj.bias, torch.Size([768])\n",
      "text_model.encoder.layers.9.self_attn.v_proj.weight, torch.Size([768, 768])\n",
      "text_model.final_layer_norm.bias, torch.Size([768])\n",
      "text_model.final_layer_norm.weight, torch.Size([768])\n",
      "text_projection.weight, torch.Size([768, 768])\n",
      "vision_model.embeddings.class_embedding, torch.Size([1024])\n",
      "vision_model.embeddings.patch_embedding.weight, torch.Size([1024, 3, 14, 14])\n",
      "vision_model.embeddings.position_embedding.weight, torch.Size([257, 1024])\n",
      "vision_model.embeddings.position_ids, torch.Size([1, 257])\n",
      "vision_model.encoder.layers.0.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.0.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.0.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.0.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.0.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.0.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.0.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.0.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.0.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.0.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.0.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.0.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.0.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.0.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.0.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.0.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.1.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.1.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.1.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.1.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.1.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.1.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.1.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.1.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.1.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.1.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.1.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.1.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.1.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.1.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.1.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.1.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.10.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.10.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.10.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.10.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.10.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.10.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.10.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.10.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.10.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.10.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.10.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.10.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.10.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.10.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.10.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.10.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.11.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.11.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.11.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.11.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.11.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.11.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.11.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.11.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.11.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.11.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.11.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.11.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.11.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.11.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.11.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.11.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.12.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.12.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.12.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.12.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.12.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.12.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.12.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.12.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.12.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.12.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.12.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.12.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.12.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.12.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.12.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.12.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.13.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.13.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.13.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.13.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.13.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.13.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.13.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.13.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.13.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.13.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.13.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.13.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.13.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.13.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.13.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.13.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.14.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.14.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.14.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.14.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.14.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.14.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.14.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.14.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.14.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.14.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.14.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.14.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.14.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.14.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.14.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.14.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.15.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.15.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.15.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.15.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.15.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.15.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.15.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.15.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.15.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.15.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.15.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.15.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.15.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.15.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.15.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.15.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.16.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.16.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.16.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.16.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.16.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.16.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.16.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.16.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.16.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.16.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.16.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.16.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.16.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.16.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.16.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.16.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.17.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.17.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.17.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.17.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.17.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.17.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.17.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.17.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.17.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.17.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.17.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.17.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.17.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.17.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.17.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.17.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.18.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.18.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.18.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.18.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.18.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.18.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.18.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.18.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.18.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.18.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.18.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.18.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.18.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.18.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.18.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.18.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.19.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.19.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.19.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.19.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.19.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.19.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.19.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.19.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.19.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.19.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.19.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.19.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.19.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.19.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.19.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.19.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.2.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.2.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.2.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.2.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.2.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.2.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.2.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.2.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.2.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.2.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.2.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.2.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.2.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.2.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.2.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.2.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.20.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.20.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.20.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.20.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.20.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.20.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.20.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.20.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.20.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.20.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.20.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.20.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.20.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.20.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.20.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.20.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.21.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.21.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.21.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.21.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.21.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.21.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.21.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.21.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.21.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.21.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.21.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.21.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.21.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.21.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.21.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.21.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.22.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.22.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.22.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.22.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.22.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.22.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.22.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.22.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.22.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.22.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.22.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.22.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.22.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.22.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.22.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.22.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.23.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.23.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.23.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.23.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.23.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.23.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.23.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.23.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.23.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.23.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.23.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.23.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.23.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.23.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.23.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.23.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.3.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.3.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.3.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.3.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.3.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.3.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.3.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.3.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.3.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.3.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.3.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.3.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.3.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.3.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.3.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.3.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.4.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.4.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.4.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.4.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.4.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.4.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.4.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.4.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.4.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.4.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.4.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.4.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.4.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.4.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.4.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.4.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.5.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.5.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.5.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.5.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.5.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.5.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.5.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.5.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.5.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.5.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.5.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.5.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.5.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.5.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.5.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.5.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.6.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.6.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.6.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.6.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.6.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.6.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.6.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.6.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.6.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.6.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.6.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.6.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.6.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.6.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.6.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.6.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.7.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.7.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.7.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.7.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.7.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.7.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.7.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.7.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.7.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.7.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.7.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.7.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.7.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.7.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.7.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.7.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.8.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.8.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.8.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.8.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.8.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.8.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.8.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.8.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.8.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.8.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.8.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.8.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.8.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.8.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.8.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.8.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.9.layer_norm1.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.9.layer_norm1.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.9.layer_norm2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.9.layer_norm2.weight, torch.Size([1024])\n",
      "vision_model.encoder.layers.9.mlp.fc1.bias, torch.Size([4096])\n",
      "vision_model.encoder.layers.9.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "vision_model.encoder.layers.9.mlp.fc2.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.9.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "vision_model.encoder.layers.9.self_attn.k_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.9.self_attn.k_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.9.self_attn.out_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.9.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.9.self_attn.q_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.9.self_attn.q_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.encoder.layers.9.self_attn.v_proj.bias, torch.Size([1024])\n",
      "vision_model.encoder.layers.9.self_attn.v_proj.weight, torch.Size([1024, 1024])\n",
      "vision_model.post_layernorm.bias, torch.Size([1024])\n",
      "vision_model.post_layernorm.weight, torch.Size([1024])\n",
      "vision_model.pre_layrnorm.bias, torch.Size([1024])\n",
      "vision_model.pre_layrnorm.weight, torch.Size([1024])\n",
      "visual_projection.weight, torch.Size([768, 1024])\n"
     ]
    }
   ],
   "source": [
    "# 对所有的 key 进行排序后打印\n",
    "for k in sorted(state_dict_clip_bin.keys()):\n",
    "    print(f\"{k}, {state_dict_clip_bin[k].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f682d8-d183-4a03-8389-a1d65e52ff39",
   "metadata": {},
   "source": [
    "# 二 模型权重转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5133b156-2681-4ad9-9cc6-baa716a51ed0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-06T02:52:44.202387Z",
     "iopub.status.busy": "2025-11-06T02:52:44.202176Z",
     "iopub.status.idle": "2025-11-06T02:52:47.474129Z",
     "shell.execute_reply": "2025-11-06T02:52:47.473668Z",
     "shell.execute_reply.started": "2025-11-06T02:52:44.202372Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_dict_clip_bin_processed = {}\n",
    "\n",
    "prefix_len = len('vision_model.')\n",
    "\n",
    "for k, v in state_dict_clip_bin.items():\n",
    "    if k.startswith('vision_model.') and not k.startswith('vision_model.post_layernorm'):\n",
    "        new_key = k[prefix_len:]\n",
    "        if new_key.startswith('encoder.'):\n",
    "            new_key = new_key.replace('encoder.', 'transformer.', 1)\n",
    "        state_dict_clip_bin_processed[new_key] = v\n",
    "\n",
    "for layer_id in range(24):\n",
    "    q_bias_key = f\"transformer.layers.{layer_id}.self_attn.q_proj.bias\"\n",
    "    k_bias_key = f\"transformer.layers.{layer_id}.self_attn.k_proj.bias\"\n",
    "    v_bias_key = f\"transformer.layers.{layer_id}.self_attn.v_proj.bias\"\n",
    "\n",
    "    q_weight_key = f\"transformer.layers.{layer_id}.self_attn.q_proj.weight\"\n",
    "    k_weight_key = f\"transformer.layers.{layer_id}.self_attn.k_proj.weight\"\n",
    "    v_weight_key = f\"transformer.layers.{layer_id}.self_attn.v_proj.weight\"\n",
    "\n",
    "    qkv_bias = torch.cat([state_dict_clip_bin_processed[q_bias_key], state_dict_clip_bin_processed[k_bias_key], state_dict_clip_bin_processed[v_bias_key]], dim=0)\n",
    "    qkv_weight = torch.cat([state_dict_clip_bin_processed[q_weight_key], state_dict_clip_bin_processed[k_weight_key], state_dict_clip_bin_processed[v_weight_key],], dim=0)\n",
    "\n",
    "    qkv_bias_key = f\"transformer.layers.{layer_id}.self_attn.qkv_proj.bias\"\n",
    "    qkv_weight_key = f\"transformer.layers.{layer_id}.self_attn.qkv_proj.weight\"\n",
    "\n",
    "    state_dict_clip_bin_processed[qkv_bias_key] = qkv_bias\n",
    "    state_dict_clip_bin_processed[qkv_weight_key] = qkv_weight\n",
    "\n",
    "    state_dict_clip_bin_processed.pop(q_bias_key, None)\n",
    "    state_dict_clip_bin_processed.pop(k_bias_key, None)\n",
    "    state_dict_clip_bin_processed.pop(v_bias_key, None)\n",
    "    state_dict_clip_bin_processed.pop(q_weight_key, None)\n",
    "    state_dict_clip_bin_processed.pop(k_weight_key, None)\n",
    "    state_dict_clip_bin_processed.pop(v_weight_key, None)\n",
    "\n",
    "\n",
    "checkpoint_clip_pth = '/mnt/workspace/models/openai/clip-vit-large-patch14/pytorch_model_processed.pth'\n",
    "torch.save(state_dict_clip_bin_processed, checkpoint_clip_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fa00ba9-9bc8-4a4d-b894-3a55db703b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T02:52:50.371891Z",
     "iopub.status.busy": "2025-11-06T02:52:50.371682Z",
     "iopub.status.idle": "2025-11-06T02:52:50.376947Z",
     "shell.execute_reply": "2025-11-06T02:52:50.376459Z",
     "shell.execute_reply.started": "2025-11-06T02:52:50.371876Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.class_embedding, torch.Size([1024])\n",
      "embeddings.patch_embedding.weight, torch.Size([1024, 3, 14, 14])\n",
      "embeddings.position_embedding.weight, torch.Size([257, 1024])\n",
      "embeddings.position_ids, torch.Size([1, 257])\n",
      "pre_layrnorm.bias, torch.Size([1024])\n",
      "pre_layrnorm.weight, torch.Size([1024])\n",
      "transformer.layers.0.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.0.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.0.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.0.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.0.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.0.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.0.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.0.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.0.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.0.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.0.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.0.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.1.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.1.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.1.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.1.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.1.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.1.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.1.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.1.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.1.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.1.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.1.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.1.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.10.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.10.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.10.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.10.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.10.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.10.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.10.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.10.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.10.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.10.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.10.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.10.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.11.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.11.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.11.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.11.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.11.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.11.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.11.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.11.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.11.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.11.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.11.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.11.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.12.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.12.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.12.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.12.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.12.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.12.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.12.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.12.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.12.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.12.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.12.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.12.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.13.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.13.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.13.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.13.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.13.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.13.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.13.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.13.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.13.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.13.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.13.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.13.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.14.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.14.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.14.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.14.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.14.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.14.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.14.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.14.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.14.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.14.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.14.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.14.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.15.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.15.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.15.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.15.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.15.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.15.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.15.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.15.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.15.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.15.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.15.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.15.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.16.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.16.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.16.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.16.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.16.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.16.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.16.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.16.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.16.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.16.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.16.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.16.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.17.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.17.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.17.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.17.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.17.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.17.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.17.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.17.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.17.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.17.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.17.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.17.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.18.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.18.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.18.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.18.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.18.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.18.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.18.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.18.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.18.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.18.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.18.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.18.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.19.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.19.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.19.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.19.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.19.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.19.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.19.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.19.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.19.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.19.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.19.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.19.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.2.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.2.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.2.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.2.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.2.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.2.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.2.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.2.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.2.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.2.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.2.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.2.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.20.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.20.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.20.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.20.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.20.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.20.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.20.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.20.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.20.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.20.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.20.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.20.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.21.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.21.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.21.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.21.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.21.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.21.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.21.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.21.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.21.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.21.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.21.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.21.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.22.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.22.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.22.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.22.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.22.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.22.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.22.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.22.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.22.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.22.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.22.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.22.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.23.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.23.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.23.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.23.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.23.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.23.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.23.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.23.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.23.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.23.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.23.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.23.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.3.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.3.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.3.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.3.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.3.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.3.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.3.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.3.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.3.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.3.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.3.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.3.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.4.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.4.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.4.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.4.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.4.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.4.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.4.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.4.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.4.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.4.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.4.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.4.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.5.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.5.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.5.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.5.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.5.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.5.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.5.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.5.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.5.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.5.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.5.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.5.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.6.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.6.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.6.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.6.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.6.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.6.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.6.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.6.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.6.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.6.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.6.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.6.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.7.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.7.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.7.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.7.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.7.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.7.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.7.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.7.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.7.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.7.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.7.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.7.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.8.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.8.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.8.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.8.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.8.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.8.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.8.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.8.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.8.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.8.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.8.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.8.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n",
      "transformer.layers.9.layer_norm1.bias, torch.Size([1024])\n",
      "transformer.layers.9.layer_norm1.weight, torch.Size([1024])\n",
      "transformer.layers.9.layer_norm2.bias, torch.Size([1024])\n",
      "transformer.layers.9.layer_norm2.weight, torch.Size([1024])\n",
      "transformer.layers.9.mlp.fc1.bias, torch.Size([4096])\n",
      "transformer.layers.9.mlp.fc1.weight, torch.Size([4096, 1024])\n",
      "transformer.layers.9.mlp.fc2.bias, torch.Size([1024])\n",
      "transformer.layers.9.mlp.fc2.weight, torch.Size([1024, 4096])\n",
      "transformer.layers.9.self_attn.out_proj.bias, torch.Size([1024])\n",
      "transformer.layers.9.self_attn.out_proj.weight, torch.Size([1024, 1024])\n",
      "transformer.layers.9.self_attn.qkv_proj.bias, torch.Size([3072])\n",
      "transformer.layers.9.self_attn.qkv_proj.weight, torch.Size([3072, 1024])\n"
     ]
    }
   ],
   "source": [
    "for k in sorted(state_dict_clip_bin_processed.keys()):\n",
    "    print(f\"{k}, {state_dict_clip_bin_processed[k].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323722e4-3038-4dec-9043-6975909b4dac",
   "metadata": {},
   "source": [
    "# 三 转换结果验证"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0649379-4df5-40bb-b8be-616cb5e4a5df",
   "metadata": {},
   "source": [
    "## 3.1 out_proj.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9a05436-d61b-4011-9c99-f16be9b793f7",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-06T03:00:50.111419Z",
     "iopub.status.busy": "2025-11-06T03:00:50.111187Z",
     "iopub.status.idle": "2025-11-06T03:00:50.115500Z",
     "shell.execute_reply": "2025-11-06T03:00:50.115085Z",
     "shell.execute_reply.started": "2025-11-06T03:00:50.111402Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0262, -0.0654,  0.0032,  ...,  0.1761, -0.0446,  0.0023])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_clip_bin['vision_model.encoder.layers.0.self_attn.out_proj.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94eb692a-32a3-4578-86ec-47680040b07a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T03:01:08.425129Z",
     "iopub.status.busy": "2025-11-06T03:01:08.424877Z",
     "iopub.status.idle": "2025-11-06T03:01:08.428770Z",
     "shell.execute_reply": "2025-11-06T03:01:08.428367Z",
     "shell.execute_reply.started": "2025-11-06T03:01:08.425109Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0262, -0.0654,  0.0032,  ...,  0.1761, -0.0446,  0.0023])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_clip_bin_processed['transformer.layers.0.self_attn.out_proj.bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e0f6c-e7e6-41fc-b1b2-1f038c47e850",
   "metadata": {},
   "source": [
    "## 3.2 out_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "333ce6a9-effc-421f-99b6-b9d1b245a0ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T03:01:19.085171Z",
     "iopub.status.busy": "2025-11-06T03:01:19.084950Z",
     "iopub.status.idle": "2025-11-06T03:01:19.089240Z",
     "shell.execute_reply": "2025-11-06T03:01:19.088797Z",
     "shell.execute_reply.started": "2025-11-06T03:01:19.085156Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.7596e-03,  8.8043e-03, -7.9422e-03,  ..., -8.6441e-03,\n",
       "         -8.7433e-03,  3.5553e-03],\n",
       "        [ 1.2077e-02,  5.8784e-03,  1.1253e-02,  ..., -3.7060e-03,\n",
       "          2.0008e-03,  3.8319e-03],\n",
       "        [-5.2032e-03,  2.6913e-03,  1.2894e-02,  ...,  6.4812e-03,\n",
       "         -3.0398e-05, -4.2796e-04],\n",
       "        ...,\n",
       "        [-4.5037e-04, -2.5063e-03, -3.2768e-03,  ..., -3.2768e-03,\n",
       "         -1.9409e-02,  9.2545e-03],\n",
       "        [-7.3624e-03,  2.8419e-03, -7.9193e-03,  ...,  4.0627e-04,\n",
       "         -1.3866e-03, -6.7186e-04],\n",
       "        [ 9.0408e-03,  1.5287e-03,  1.6737e-03,  ...,  2.4242e-03,\n",
       "         -3.7575e-03,  4.9667e-03]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_clip_bin['vision_model.encoder.layers.0.self_attn.out_proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adfceb9a-69dd-4de9-b5df-77681782a3f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T03:01:25.811256Z",
     "iopub.status.busy": "2025-11-06T03:01:25.811025Z",
     "iopub.status.idle": "2025-11-06T03:01:25.815127Z",
     "shell.execute_reply": "2025-11-06T03:01:25.814746Z",
     "shell.execute_reply.started": "2025-11-06T03:01:25.811241Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.7596e-03,  8.8043e-03, -7.9422e-03,  ..., -8.6441e-03,\n",
       "         -8.7433e-03,  3.5553e-03],\n",
       "        [ 1.2077e-02,  5.8784e-03,  1.1253e-02,  ..., -3.7060e-03,\n",
       "          2.0008e-03,  3.8319e-03],\n",
       "        [-5.2032e-03,  2.6913e-03,  1.2894e-02,  ...,  6.4812e-03,\n",
       "         -3.0398e-05, -4.2796e-04],\n",
       "        ...,\n",
       "        [-4.5037e-04, -2.5063e-03, -3.2768e-03,  ..., -3.2768e-03,\n",
       "         -1.9409e-02,  9.2545e-03],\n",
       "        [-7.3624e-03,  2.8419e-03, -7.9193e-03,  ...,  4.0627e-04,\n",
       "         -1.3866e-03, -6.7186e-04],\n",
       "        [ 9.0408e-03,  1.5287e-03,  1.6737e-03,  ...,  2.4242e-03,\n",
       "         -3.7575e-03,  4.9667e-03]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_clip_bin_processed['transformer.layers.0.self_attn.out_proj.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e2a8d-3712-48d6-b3b0-820488b7124f",
   "metadata": {},
   "source": [
    "## 3.3 qkv_proj.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fd42db4-1d7f-4b68-8f12-c00039751a6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T03:01:50.778313Z",
     "iopub.status.busy": "2025-11-06T03:01:50.778095Z",
     "iopub.status.idle": "2025-11-06T03:01:50.781745Z",
     "shell.execute_reply": "2025-11-06T03:01:50.781371Z",
     "shell.execute_reply.started": "2025-11-06T03:01:50.778297Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5674, -1.6143, -0.8208,  ..., -1.2832, -0.0975,  0.7827])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_clip_bin['vision_model.encoder.layers.0.self_attn.q_proj.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45bfd9b7-4fe2-4480-8983-7828b3fc6325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T03:01:58.163245Z",
     "iopub.status.busy": "2025-11-06T03:01:58.163006Z",
     "iopub.status.idle": "2025-11-06T03:01:58.166920Z",
     "shell.execute_reply": "2025-11-06T03:01:58.166343Z",
     "shell.execute_reply.started": "2025-11-06T03:01:58.163227Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0277,  0.0222, -0.0353,  ...,  0.0115,  0.0107, -0.0043])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_clip_bin['vision_model.encoder.layers.0.self_attn.v_proj.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3260936-d87c-4cd5-b5f6-8ed8039be397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T03:02:05.057402Z",
     "iopub.status.busy": "2025-11-06T03:02:05.057193Z",
     "iopub.status.idle": "2025-11-06T03:02:05.060883Z",
     "shell.execute_reply": "2025-11-06T03:02:05.060484Z",
     "shell.execute_reply.started": "2025-11-06T03:02:05.057388Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5674, -1.6143, -0.8208,  ...,  0.0115,  0.0107, -0.0043])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_clip_bin_processed['transformer.layers.0.self_attn.qkv_proj.bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd32401e-6db5-41c0-b81f-b5b90f5a4413",
   "metadata": {},
   "source": [
    "## 3.4 qkv_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4585338c-aa76-4dde-bf85-7fe8c15dcf96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T03:02:22.914251Z",
     "iopub.status.busy": "2025-11-06T03:02:22.914022Z",
     "iopub.status.idle": "2025-11-06T03:02:22.918249Z",
     "shell.execute_reply": "2025-11-06T03:02:22.917852Z",
     "shell.execute_reply.started": "2025-11-06T03:02:22.914231Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0632e-05, -1.6510e-04, -7.0930e-05,  ...,  4.5090e-03,\n",
       "         -2.9160e-02, -7.8201e-05],\n",
       "        [-1.3733e-04,  1.2165e-04,  4.2319e-05,  ..., -1.6594e-03,\n",
       "          3.1433e-02,  7.4446e-05],\n",
       "        [ 4.8018e-04,  7.7963e-04, -1.0991e-04,  ..., -1.6846e-02,\n",
       "          4.2999e-02,  1.5199e-04],\n",
       "        ...,\n",
       "        [ 6.8367e-05,  8.2791e-05,  8.7738e-05,  ..., -3.9940e-03,\n",
       "          1.2596e-02, -3.9220e-05],\n",
       "        [-1.5414e-04,  3.5167e-06, -2.7108e-04,  ..., -1.5259e-04,\n",
       "         -3.3212e-04,  1.6868e-05],\n",
       "        [-1.3053e-04,  8.8096e-05,  5.4955e-05,  ..., -1.4862e-02,\n",
       "         -1.4143e-03,  4.3333e-05]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_clip_bin['vision_model.encoder.layers.0.self_attn.q_proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f187b231-cd00-40d6-b08c-5dc05428e735",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T03:02:29.597574Z",
     "iopub.status.busy": "2025-11-06T03:02:29.597368Z",
     "iopub.status.idle": "2025-11-06T03:02:29.601498Z",
     "shell.execute_reply": "2025-11-06T03:02:29.601093Z",
     "shell.execute_reply.started": "2025-11-06T03:02:29.597559Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.6023e-05, -1.6069e-04, -5.7364e-04,  ...,  3.5210e-03,\n",
       "         -1.0292e-02,  9.0539e-05],\n",
       "        [-3.4380e-04, -1.9860e-04,  5.0724e-05,  ...,  9.1410e-04,\n",
       "          9.4299e-03, -8.7619e-05],\n",
       "        [ 5.0259e-04, -8.7440e-05,  2.2519e-04,  ..., -1.0691e-03,\n",
       "         -1.9806e-02, -1.0806e-04],\n",
       "        ...,\n",
       "        [ 2.1267e-04,  4.1032e-04, -7.2420e-05,  ...,  4.8027e-03,\n",
       "         -1.7338e-03, -6.6102e-05],\n",
       "        [ 3.0518e-04, -4.4405e-05, -2.2709e-04,  ...,  1.1551e-02,\n",
       "          3.3436e-03,  7.4685e-05],\n",
       "        [-2.8849e-05,  4.5919e-04,  9.3341e-05,  ..., -1.1314e-02,\n",
       "          3.7670e-03, -7.7844e-05]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_clip_bin['vision_model.encoder.layers.0.self_attn.v_proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1aea686d-714b-49d2-8c20-a561b319af36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T03:02:36.084622Z",
     "iopub.status.busy": "2025-11-06T03:02:36.084401Z",
     "iopub.status.idle": "2025-11-06T03:02:36.089597Z",
     "shell.execute_reply": "2025-11-06T03:02:36.089094Z",
     "shell.execute_reply.started": "2025-11-06T03:02:36.084604Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0632e-05, -1.6510e-04, -7.0930e-05,  ...,  4.5090e-03,\n",
       "         -2.9160e-02, -7.8201e-05],\n",
       "        [-1.3733e-04,  1.2165e-04,  4.2319e-05,  ..., -1.6594e-03,\n",
       "          3.1433e-02,  7.4446e-05],\n",
       "        [ 4.8018e-04,  7.7963e-04, -1.0991e-04,  ..., -1.6846e-02,\n",
       "          4.2999e-02,  1.5199e-04],\n",
       "        ...,\n",
       "        [ 2.1267e-04,  4.1032e-04, -7.2420e-05,  ...,  4.8027e-03,\n",
       "         -1.7338e-03, -6.6102e-05],\n",
       "        [ 3.0518e-04, -4.4405e-05, -2.2709e-04,  ...,  1.1551e-02,\n",
       "          3.3436e-03,  7.4685e-05],\n",
       "        [-2.8849e-05,  4.5919e-04,  9.3341e-05,  ..., -1.1314e-02,\n",
       "          3.7670e-03, -7.7844e-05]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_clip_bin_processed['transformer.layers.0.self_attn.qkv_proj.weight']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
