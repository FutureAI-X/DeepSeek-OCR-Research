{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0078e70-460d-455f-ae07-138c78ecd687",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-06T07:14:49.209878Z",
     "iopub.status.busy": "2025-11-06T07:14:49.209687Z",
     "iopub.status.idle": "2025-11-06T07:14:53.254581Z",
     "shell.execute_reply": "2025-11-06T07:14:53.254071Z",
     "shell.execute_reply.started": "2025-11-06T07:14:49.209863Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from modeling_deepseek_ocr import DeepseekOCRForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824b7f33-a8bc-49d4-8ad0-f7766fee0a3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T07:14:53.981679Z",
     "iopub.status.busy": "2025-11-06T07:14:53.981310Z",
     "iopub.status.idle": "2025-11-06T07:14:53.984159Z",
     "shell.execute_reply": "2025-11-06T07:14:53.983699Z",
     "shell.execute_reply.started": "2025-11-06T07:14:53.981662Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qwen_path = \"/mnt/workspace/models/Qwen/Qwen3-0.6B\"\n",
    "sam_file = \"/mnt/workspace/models/facebook/sam-vit-base/sam_vit_b_01ec64_processed.pth\"\n",
    "clip_file = \"/mnt/workspace/models/openai/clip-vit-large-patch14/pytorch_model_processed.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9da2bf3-e481-4ee2-882b-5566c97b33f7",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-06T07:14:55.438911Z",
     "iopub.status.busy": "2025-11-06T07:14:55.438696Z",
     "iopub.status.idle": "2025-11-06T07:15:06.523923Z",
     "shell.execute_reply": "2025-11-06T07:15:06.523487Z",
     "shell.execute_reply.started": "2025-11-06T07:14:55.438897Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type qwen3 to instantiate a model of type DeepSeekOCR. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at /mnt/workspace/models/Qwen/Qwen3-0.6B and are newly initialized: ['model.image_newline', 'model.projector.bias', 'model.projector.weight', 'model.sam_model.blocks.0.attn.proj.bias', 'model.sam_model.blocks.0.attn.proj.weight', 'model.sam_model.blocks.0.attn.qkv.bias', 'model.sam_model.blocks.0.attn.qkv.weight', 'model.sam_model.blocks.0.attn.rel_pos_h', 'model.sam_model.blocks.0.attn.rel_pos_w', 'model.sam_model.blocks.0.mlp.lin1.bias', 'model.sam_model.blocks.0.mlp.lin1.weight', 'model.sam_model.blocks.0.mlp.lin2.bias', 'model.sam_model.blocks.0.mlp.lin2.weight', 'model.sam_model.blocks.0.norm1.bias', 'model.sam_model.blocks.0.norm1.weight', 'model.sam_model.blocks.0.norm2.bias', 'model.sam_model.blocks.0.norm2.weight', 'model.sam_model.blocks.1.attn.proj.bias', 'model.sam_model.blocks.1.attn.proj.weight', 'model.sam_model.blocks.1.attn.qkv.bias', 'model.sam_model.blocks.1.attn.qkv.weight', 'model.sam_model.blocks.1.attn.rel_pos_h', 'model.sam_model.blocks.1.attn.rel_pos_w', 'model.sam_model.blocks.1.mlp.lin1.bias', 'model.sam_model.blocks.1.mlp.lin1.weight', 'model.sam_model.blocks.1.mlp.lin2.bias', 'model.sam_model.blocks.1.mlp.lin2.weight', 'model.sam_model.blocks.1.norm1.bias', 'model.sam_model.blocks.1.norm1.weight', 'model.sam_model.blocks.1.norm2.bias', 'model.sam_model.blocks.1.norm2.weight', 'model.sam_model.blocks.10.attn.proj.bias', 'model.sam_model.blocks.10.attn.proj.weight', 'model.sam_model.blocks.10.attn.qkv.bias', 'model.sam_model.blocks.10.attn.qkv.weight', 'model.sam_model.blocks.10.attn.rel_pos_h', 'model.sam_model.blocks.10.attn.rel_pos_w', 'model.sam_model.blocks.10.mlp.lin1.bias', 'model.sam_model.blocks.10.mlp.lin1.weight', 'model.sam_model.blocks.10.mlp.lin2.bias', 'model.sam_model.blocks.10.mlp.lin2.weight', 'model.sam_model.blocks.10.norm1.bias', 'model.sam_model.blocks.10.norm1.weight', 'model.sam_model.blocks.10.norm2.bias', 'model.sam_model.blocks.10.norm2.weight', 'model.sam_model.blocks.11.attn.proj.bias', 'model.sam_model.blocks.11.attn.proj.weight', 'model.sam_model.blocks.11.attn.qkv.bias', 'model.sam_model.blocks.11.attn.qkv.weight', 'model.sam_model.blocks.11.attn.rel_pos_h', 'model.sam_model.blocks.11.attn.rel_pos_w', 'model.sam_model.blocks.11.mlp.lin1.bias', 'model.sam_model.blocks.11.mlp.lin1.weight', 'model.sam_model.blocks.11.mlp.lin2.bias', 'model.sam_model.blocks.11.mlp.lin2.weight', 'model.sam_model.blocks.11.norm1.bias', 'model.sam_model.blocks.11.norm1.weight', 'model.sam_model.blocks.11.norm2.bias', 'model.sam_model.blocks.11.norm2.weight', 'model.sam_model.blocks.2.attn.proj.bias', 'model.sam_model.blocks.2.attn.proj.weight', 'model.sam_model.blocks.2.attn.qkv.bias', 'model.sam_model.blocks.2.attn.qkv.weight', 'model.sam_model.blocks.2.attn.rel_pos_h', 'model.sam_model.blocks.2.attn.rel_pos_w', 'model.sam_model.blocks.2.mlp.lin1.bias', 'model.sam_model.blocks.2.mlp.lin1.weight', 'model.sam_model.blocks.2.mlp.lin2.bias', 'model.sam_model.blocks.2.mlp.lin2.weight', 'model.sam_model.blocks.2.norm1.bias', 'model.sam_model.blocks.2.norm1.weight', 'model.sam_model.blocks.2.norm2.bias', 'model.sam_model.blocks.2.norm2.weight', 'model.sam_model.blocks.3.attn.proj.bias', 'model.sam_model.blocks.3.attn.proj.weight', 'model.sam_model.blocks.3.attn.qkv.bias', 'model.sam_model.blocks.3.attn.qkv.weight', 'model.sam_model.blocks.3.attn.rel_pos_h', 'model.sam_model.blocks.3.attn.rel_pos_w', 'model.sam_model.blocks.3.mlp.lin1.bias', 'model.sam_model.blocks.3.mlp.lin1.weight', 'model.sam_model.blocks.3.mlp.lin2.bias', 'model.sam_model.blocks.3.mlp.lin2.weight', 'model.sam_model.blocks.3.norm1.bias', 'model.sam_model.blocks.3.norm1.weight', 'model.sam_model.blocks.3.norm2.bias', 'model.sam_model.blocks.3.norm2.weight', 'model.sam_model.blocks.4.attn.proj.bias', 'model.sam_model.blocks.4.attn.proj.weight', 'model.sam_model.blocks.4.attn.qkv.bias', 'model.sam_model.blocks.4.attn.qkv.weight', 'model.sam_model.blocks.4.attn.rel_pos_h', 'model.sam_model.blocks.4.attn.rel_pos_w', 'model.sam_model.blocks.4.mlp.lin1.bias', 'model.sam_model.blocks.4.mlp.lin1.weight', 'model.sam_model.blocks.4.mlp.lin2.bias', 'model.sam_model.blocks.4.mlp.lin2.weight', 'model.sam_model.blocks.4.norm1.bias', 'model.sam_model.blocks.4.norm1.weight', 'model.sam_model.blocks.4.norm2.bias', 'model.sam_model.blocks.4.norm2.weight', 'model.sam_model.blocks.5.attn.proj.bias', 'model.sam_model.blocks.5.attn.proj.weight', 'model.sam_model.blocks.5.attn.qkv.bias', 'model.sam_model.blocks.5.attn.qkv.weight', 'model.sam_model.blocks.5.attn.rel_pos_h', 'model.sam_model.blocks.5.attn.rel_pos_w', 'model.sam_model.blocks.5.mlp.lin1.bias', 'model.sam_model.blocks.5.mlp.lin1.weight', 'model.sam_model.blocks.5.mlp.lin2.bias', 'model.sam_model.blocks.5.mlp.lin2.weight', 'model.sam_model.blocks.5.norm1.bias', 'model.sam_model.blocks.5.norm1.weight', 'model.sam_model.blocks.5.norm2.bias', 'model.sam_model.blocks.5.norm2.weight', 'model.sam_model.blocks.6.attn.proj.bias', 'model.sam_model.blocks.6.attn.proj.weight', 'model.sam_model.blocks.6.attn.qkv.bias', 'model.sam_model.blocks.6.attn.qkv.weight', 'model.sam_model.blocks.6.attn.rel_pos_h', 'model.sam_model.blocks.6.attn.rel_pos_w', 'model.sam_model.blocks.6.mlp.lin1.bias', 'model.sam_model.blocks.6.mlp.lin1.weight', 'model.sam_model.blocks.6.mlp.lin2.bias', 'model.sam_model.blocks.6.mlp.lin2.weight', 'model.sam_model.blocks.6.norm1.bias', 'model.sam_model.blocks.6.norm1.weight', 'model.sam_model.blocks.6.norm2.bias', 'model.sam_model.blocks.6.norm2.weight', 'model.sam_model.blocks.7.attn.proj.bias', 'model.sam_model.blocks.7.attn.proj.weight', 'model.sam_model.blocks.7.attn.qkv.bias', 'model.sam_model.blocks.7.attn.qkv.weight', 'model.sam_model.blocks.7.attn.rel_pos_h', 'model.sam_model.blocks.7.attn.rel_pos_w', 'model.sam_model.blocks.7.mlp.lin1.bias', 'model.sam_model.blocks.7.mlp.lin1.weight', 'model.sam_model.blocks.7.mlp.lin2.bias', 'model.sam_model.blocks.7.mlp.lin2.weight', 'model.sam_model.blocks.7.norm1.bias', 'model.sam_model.blocks.7.norm1.weight', 'model.sam_model.blocks.7.norm2.bias', 'model.sam_model.blocks.7.norm2.weight', 'model.sam_model.blocks.8.attn.proj.bias', 'model.sam_model.blocks.8.attn.proj.weight', 'model.sam_model.blocks.8.attn.qkv.bias', 'model.sam_model.blocks.8.attn.qkv.weight', 'model.sam_model.blocks.8.attn.rel_pos_h', 'model.sam_model.blocks.8.attn.rel_pos_w', 'model.sam_model.blocks.8.mlp.lin1.bias', 'model.sam_model.blocks.8.mlp.lin1.weight', 'model.sam_model.blocks.8.mlp.lin2.bias', 'model.sam_model.blocks.8.mlp.lin2.weight', 'model.sam_model.blocks.8.norm1.bias', 'model.sam_model.blocks.8.norm1.weight', 'model.sam_model.blocks.8.norm2.bias', 'model.sam_model.blocks.8.norm2.weight', 'model.sam_model.blocks.9.attn.proj.bias', 'model.sam_model.blocks.9.attn.proj.weight', 'model.sam_model.blocks.9.attn.qkv.bias', 'model.sam_model.blocks.9.attn.qkv.weight', 'model.sam_model.blocks.9.attn.rel_pos_h', 'model.sam_model.blocks.9.attn.rel_pos_w', 'model.sam_model.blocks.9.mlp.lin1.bias', 'model.sam_model.blocks.9.mlp.lin1.weight', 'model.sam_model.blocks.9.mlp.lin2.bias', 'model.sam_model.blocks.9.mlp.lin2.weight', 'model.sam_model.blocks.9.norm1.bias', 'model.sam_model.blocks.9.norm1.weight', 'model.sam_model.blocks.9.norm2.bias', 'model.sam_model.blocks.9.norm2.weight', 'model.sam_model.neck.0.weight', 'model.sam_model.neck.1.bias', 'model.sam_model.neck.1.weight', 'model.sam_model.neck.2.weight', 'model.sam_model.neck.3.bias', 'model.sam_model.neck.3.weight', 'model.sam_model.net_2.weight', 'model.sam_model.net_3.weight', 'model.sam_model.patch_embed.proj.bias', 'model.sam_model.patch_embed.proj.weight', 'model.sam_model.pos_embed', 'model.view_seperator', 'model.vision_model.embeddings.class_embedding', 'model.vision_model.embeddings.patch_embedding.weight', 'model.vision_model.embeddings.position_embedding.weight', 'model.vision_model.embeddings.position_ids', 'model.vision_model.pre_layrnorm.bias', 'model.vision_model.pre_layrnorm.weight', 'model.vision_model.transformer.layers.0.layer_norm1.bias', 'model.vision_model.transformer.layers.0.layer_norm1.weight', 'model.vision_model.transformer.layers.0.layer_norm2.bias', 'model.vision_model.transformer.layers.0.layer_norm2.weight', 'model.vision_model.transformer.layers.0.mlp.fc1.bias', 'model.vision_model.transformer.layers.0.mlp.fc1.weight', 'model.vision_model.transformer.layers.0.mlp.fc2.bias', 'model.vision_model.transformer.layers.0.mlp.fc2.weight', 'model.vision_model.transformer.layers.0.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.0.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.0.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.0.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.1.layer_norm1.bias', 'model.vision_model.transformer.layers.1.layer_norm1.weight', 'model.vision_model.transformer.layers.1.layer_norm2.bias', 'model.vision_model.transformer.layers.1.layer_norm2.weight', 'model.vision_model.transformer.layers.1.mlp.fc1.bias', 'model.vision_model.transformer.layers.1.mlp.fc1.weight', 'model.vision_model.transformer.layers.1.mlp.fc2.bias', 'model.vision_model.transformer.layers.1.mlp.fc2.weight', 'model.vision_model.transformer.layers.1.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.1.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.1.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.1.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.10.layer_norm1.bias', 'model.vision_model.transformer.layers.10.layer_norm1.weight', 'model.vision_model.transformer.layers.10.layer_norm2.bias', 'model.vision_model.transformer.layers.10.layer_norm2.weight', 'model.vision_model.transformer.layers.10.mlp.fc1.bias', 'model.vision_model.transformer.layers.10.mlp.fc1.weight', 'model.vision_model.transformer.layers.10.mlp.fc2.bias', 'model.vision_model.transformer.layers.10.mlp.fc2.weight', 'model.vision_model.transformer.layers.10.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.10.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.10.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.10.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.11.layer_norm1.bias', 'model.vision_model.transformer.layers.11.layer_norm1.weight', 'model.vision_model.transformer.layers.11.layer_norm2.bias', 'model.vision_model.transformer.layers.11.layer_norm2.weight', 'model.vision_model.transformer.layers.11.mlp.fc1.bias', 'model.vision_model.transformer.layers.11.mlp.fc1.weight', 'model.vision_model.transformer.layers.11.mlp.fc2.bias', 'model.vision_model.transformer.layers.11.mlp.fc2.weight', 'model.vision_model.transformer.layers.11.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.11.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.11.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.11.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.12.layer_norm1.bias', 'model.vision_model.transformer.layers.12.layer_norm1.weight', 'model.vision_model.transformer.layers.12.layer_norm2.bias', 'model.vision_model.transformer.layers.12.layer_norm2.weight', 'model.vision_model.transformer.layers.12.mlp.fc1.bias', 'model.vision_model.transformer.layers.12.mlp.fc1.weight', 'model.vision_model.transformer.layers.12.mlp.fc2.bias', 'model.vision_model.transformer.layers.12.mlp.fc2.weight', 'model.vision_model.transformer.layers.12.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.12.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.12.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.12.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.13.layer_norm1.bias', 'model.vision_model.transformer.layers.13.layer_norm1.weight', 'model.vision_model.transformer.layers.13.layer_norm2.bias', 'model.vision_model.transformer.layers.13.layer_norm2.weight', 'model.vision_model.transformer.layers.13.mlp.fc1.bias', 'model.vision_model.transformer.layers.13.mlp.fc1.weight', 'model.vision_model.transformer.layers.13.mlp.fc2.bias', 'model.vision_model.transformer.layers.13.mlp.fc2.weight', 'model.vision_model.transformer.layers.13.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.13.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.13.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.13.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.14.layer_norm1.bias', 'model.vision_model.transformer.layers.14.layer_norm1.weight', 'model.vision_model.transformer.layers.14.layer_norm2.bias', 'model.vision_model.transformer.layers.14.layer_norm2.weight', 'model.vision_model.transformer.layers.14.mlp.fc1.bias', 'model.vision_model.transformer.layers.14.mlp.fc1.weight', 'model.vision_model.transformer.layers.14.mlp.fc2.bias', 'model.vision_model.transformer.layers.14.mlp.fc2.weight', 'model.vision_model.transformer.layers.14.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.14.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.14.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.14.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.15.layer_norm1.bias', 'model.vision_model.transformer.layers.15.layer_norm1.weight', 'model.vision_model.transformer.layers.15.layer_norm2.bias', 'model.vision_model.transformer.layers.15.layer_norm2.weight', 'model.vision_model.transformer.layers.15.mlp.fc1.bias', 'model.vision_model.transformer.layers.15.mlp.fc1.weight', 'model.vision_model.transformer.layers.15.mlp.fc2.bias', 'model.vision_model.transformer.layers.15.mlp.fc2.weight', 'model.vision_model.transformer.layers.15.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.15.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.15.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.15.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.16.layer_norm1.bias', 'model.vision_model.transformer.layers.16.layer_norm1.weight', 'model.vision_model.transformer.layers.16.layer_norm2.bias', 'model.vision_model.transformer.layers.16.layer_norm2.weight', 'model.vision_model.transformer.layers.16.mlp.fc1.bias', 'model.vision_model.transformer.layers.16.mlp.fc1.weight', 'model.vision_model.transformer.layers.16.mlp.fc2.bias', 'model.vision_model.transformer.layers.16.mlp.fc2.weight', 'model.vision_model.transformer.layers.16.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.16.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.16.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.16.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.17.layer_norm1.bias', 'model.vision_model.transformer.layers.17.layer_norm1.weight', 'model.vision_model.transformer.layers.17.layer_norm2.bias', 'model.vision_model.transformer.layers.17.layer_norm2.weight', 'model.vision_model.transformer.layers.17.mlp.fc1.bias', 'model.vision_model.transformer.layers.17.mlp.fc1.weight', 'model.vision_model.transformer.layers.17.mlp.fc2.bias', 'model.vision_model.transformer.layers.17.mlp.fc2.weight', 'model.vision_model.transformer.layers.17.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.17.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.17.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.17.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.18.layer_norm1.bias', 'model.vision_model.transformer.layers.18.layer_norm1.weight', 'model.vision_model.transformer.layers.18.layer_norm2.bias', 'model.vision_model.transformer.layers.18.layer_norm2.weight', 'model.vision_model.transformer.layers.18.mlp.fc1.bias', 'model.vision_model.transformer.layers.18.mlp.fc1.weight', 'model.vision_model.transformer.layers.18.mlp.fc2.bias', 'model.vision_model.transformer.layers.18.mlp.fc2.weight', 'model.vision_model.transformer.layers.18.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.18.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.18.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.18.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.19.layer_norm1.bias', 'model.vision_model.transformer.layers.19.layer_norm1.weight', 'model.vision_model.transformer.layers.19.layer_norm2.bias', 'model.vision_model.transformer.layers.19.layer_norm2.weight', 'model.vision_model.transformer.layers.19.mlp.fc1.bias', 'model.vision_model.transformer.layers.19.mlp.fc1.weight', 'model.vision_model.transformer.layers.19.mlp.fc2.bias', 'model.vision_model.transformer.layers.19.mlp.fc2.weight', 'model.vision_model.transformer.layers.19.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.19.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.19.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.19.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.2.layer_norm1.bias', 'model.vision_model.transformer.layers.2.layer_norm1.weight', 'model.vision_model.transformer.layers.2.layer_norm2.bias', 'model.vision_model.transformer.layers.2.layer_norm2.weight', 'model.vision_model.transformer.layers.2.mlp.fc1.bias', 'model.vision_model.transformer.layers.2.mlp.fc1.weight', 'model.vision_model.transformer.layers.2.mlp.fc2.bias', 'model.vision_model.transformer.layers.2.mlp.fc2.weight', 'model.vision_model.transformer.layers.2.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.2.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.2.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.2.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.20.layer_norm1.bias', 'model.vision_model.transformer.layers.20.layer_norm1.weight', 'model.vision_model.transformer.layers.20.layer_norm2.bias', 'model.vision_model.transformer.layers.20.layer_norm2.weight', 'model.vision_model.transformer.layers.20.mlp.fc1.bias', 'model.vision_model.transformer.layers.20.mlp.fc1.weight', 'model.vision_model.transformer.layers.20.mlp.fc2.bias', 'model.vision_model.transformer.layers.20.mlp.fc2.weight', 'model.vision_model.transformer.layers.20.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.20.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.20.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.20.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.21.layer_norm1.bias', 'model.vision_model.transformer.layers.21.layer_norm1.weight', 'model.vision_model.transformer.layers.21.layer_norm2.bias', 'model.vision_model.transformer.layers.21.layer_norm2.weight', 'model.vision_model.transformer.layers.21.mlp.fc1.bias', 'model.vision_model.transformer.layers.21.mlp.fc1.weight', 'model.vision_model.transformer.layers.21.mlp.fc2.bias', 'model.vision_model.transformer.layers.21.mlp.fc2.weight', 'model.vision_model.transformer.layers.21.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.21.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.21.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.21.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.22.layer_norm1.bias', 'model.vision_model.transformer.layers.22.layer_norm1.weight', 'model.vision_model.transformer.layers.22.layer_norm2.bias', 'model.vision_model.transformer.layers.22.layer_norm2.weight', 'model.vision_model.transformer.layers.22.mlp.fc1.bias', 'model.vision_model.transformer.layers.22.mlp.fc1.weight', 'model.vision_model.transformer.layers.22.mlp.fc2.bias', 'model.vision_model.transformer.layers.22.mlp.fc2.weight', 'model.vision_model.transformer.layers.22.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.22.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.22.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.22.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.23.layer_norm1.bias', 'model.vision_model.transformer.layers.23.layer_norm1.weight', 'model.vision_model.transformer.layers.23.layer_norm2.bias', 'model.vision_model.transformer.layers.23.layer_norm2.weight', 'model.vision_model.transformer.layers.23.mlp.fc1.bias', 'model.vision_model.transformer.layers.23.mlp.fc1.weight', 'model.vision_model.transformer.layers.23.mlp.fc2.bias', 'model.vision_model.transformer.layers.23.mlp.fc2.weight', 'model.vision_model.transformer.layers.23.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.23.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.23.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.23.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.3.layer_norm1.bias', 'model.vision_model.transformer.layers.3.layer_norm1.weight', 'model.vision_model.transformer.layers.3.layer_norm2.bias', 'model.vision_model.transformer.layers.3.layer_norm2.weight', 'model.vision_model.transformer.layers.3.mlp.fc1.bias', 'model.vision_model.transformer.layers.3.mlp.fc1.weight', 'model.vision_model.transformer.layers.3.mlp.fc2.bias', 'model.vision_model.transformer.layers.3.mlp.fc2.weight', 'model.vision_model.transformer.layers.3.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.3.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.3.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.3.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.4.layer_norm1.bias', 'model.vision_model.transformer.layers.4.layer_norm1.weight', 'model.vision_model.transformer.layers.4.layer_norm2.bias', 'model.vision_model.transformer.layers.4.layer_norm2.weight', 'model.vision_model.transformer.layers.4.mlp.fc1.bias', 'model.vision_model.transformer.layers.4.mlp.fc1.weight', 'model.vision_model.transformer.layers.4.mlp.fc2.bias', 'model.vision_model.transformer.layers.4.mlp.fc2.weight', 'model.vision_model.transformer.layers.4.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.4.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.4.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.4.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.5.layer_norm1.bias', 'model.vision_model.transformer.layers.5.layer_norm1.weight', 'model.vision_model.transformer.layers.5.layer_norm2.bias', 'model.vision_model.transformer.layers.5.layer_norm2.weight', 'model.vision_model.transformer.layers.5.mlp.fc1.bias', 'model.vision_model.transformer.layers.5.mlp.fc1.weight', 'model.vision_model.transformer.layers.5.mlp.fc2.bias', 'model.vision_model.transformer.layers.5.mlp.fc2.weight', 'model.vision_model.transformer.layers.5.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.5.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.5.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.5.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.6.layer_norm1.bias', 'model.vision_model.transformer.layers.6.layer_norm1.weight', 'model.vision_model.transformer.layers.6.layer_norm2.bias', 'model.vision_model.transformer.layers.6.layer_norm2.weight', 'model.vision_model.transformer.layers.6.mlp.fc1.bias', 'model.vision_model.transformer.layers.6.mlp.fc1.weight', 'model.vision_model.transformer.layers.6.mlp.fc2.bias', 'model.vision_model.transformer.layers.6.mlp.fc2.weight', 'model.vision_model.transformer.layers.6.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.6.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.6.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.6.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.7.layer_norm1.bias', 'model.vision_model.transformer.layers.7.layer_norm1.weight', 'model.vision_model.transformer.layers.7.layer_norm2.bias', 'model.vision_model.transformer.layers.7.layer_norm2.weight', 'model.vision_model.transformer.layers.7.mlp.fc1.bias', 'model.vision_model.transformer.layers.7.mlp.fc1.weight', 'model.vision_model.transformer.layers.7.mlp.fc2.bias', 'model.vision_model.transformer.layers.7.mlp.fc2.weight', 'model.vision_model.transformer.layers.7.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.7.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.7.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.7.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.8.layer_norm1.bias', 'model.vision_model.transformer.layers.8.layer_norm1.weight', 'model.vision_model.transformer.layers.8.layer_norm2.bias', 'model.vision_model.transformer.layers.8.layer_norm2.weight', 'model.vision_model.transformer.layers.8.mlp.fc1.bias', 'model.vision_model.transformer.layers.8.mlp.fc1.weight', 'model.vision_model.transformer.layers.8.mlp.fc2.bias', 'model.vision_model.transformer.layers.8.mlp.fc2.weight', 'model.vision_model.transformer.layers.8.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.8.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.8.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.8.self_attn.qkv_proj.weight', 'model.vision_model.transformer.layers.9.layer_norm1.bias', 'model.vision_model.transformer.layers.9.layer_norm1.weight', 'model.vision_model.transformer.layers.9.layer_norm2.bias', 'model.vision_model.transformer.layers.9.layer_norm2.weight', 'model.vision_model.transformer.layers.9.mlp.fc1.bias', 'model.vision_model.transformer.layers.9.mlp.fc1.weight', 'model.vision_model.transformer.layers.9.mlp.fc2.bias', 'model.vision_model.transformer.layers.9.mlp.fc2.weight', 'model.vision_model.transformer.layers.9.self_attn.out_proj.bias', 'model.vision_model.transformer.layers.9.self_attn.out_proj.weight', 'model.vision_model.transformer.layers.9.self_attn.qkv_proj.bias', 'model.vision_model.transformer.layers.9.self_attn.qkv_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(qwen_path)\n",
    "model = DeepseekOCRForCausalLM.from_pretrained(\n",
    "    qwen_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f1a5ba-558c-4099-b841-19d73022edf6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-06T07:15:11.452959Z",
     "iopub.status.busy": "2025-11-06T07:15:11.452731Z",
     "iopub.status.idle": "2025-11-06T07:15:11.692312Z",
     "shell.execute_reply": "2025-11-06T07:15:11.691815Z",
     "shell.execute_reply.started": "2025-11-06T07:15:11.452938Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['net_2.weight', 'net_3.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_state_dict = torch.load(sam_file)\n",
    "model.model.sam_model.load_state_dict(sam_state_dict, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82af483-5e2d-4d85-92e6-3197d9b81baa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T07:15:14.327613Z",
     "iopub.status.busy": "2025-11-06T07:15:14.327406Z",
     "iopub.status.idle": "2025-11-06T07:15:15.087427Z",
     "shell.execute_reply": "2025-11-06T07:15:15.086744Z",
     "shell.execute_reply.started": "2025-11-06T07:15:14.327599Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_state_dict = torch.load(clip_file)\n",
    "model.model.vision_model.load_state_dict(clip_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb9430a4-4a50-4abc-931a-1aece2aeabf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T07:15:17.241244Z",
     "iopub.status.busy": "2025-11-06T07:15:17.241014Z",
     "iopub.status.idle": "2025-11-06T07:15:17.250470Z",
     "shell.execute_reply": "2025-11-06T07:15:17.249975Z",
     "shell.execute_reply.started": "2025-11-06T07:15:17.241227Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0038,  0.0193,  0.0006,  ...,  0.0039, -0.0237, -0.0004],\n",
       "        [-0.0127,  0.0071, -0.0154,  ..., -0.0171, -0.0293, -0.0258],\n",
       "        [ 0.0105, -0.0062, -0.0084,  ..., -0.0066,  0.0157, -0.0396],\n",
       "        ...,\n",
       "        [-0.0033, -0.0087,  0.0045,  ..., -0.0479, -0.0259, -0.0090],\n",
       "        [ 0.0052, -0.0121, -0.0056,  ...,  0.0063,  0.0052,  0.0098],\n",
       "        [ 0.0094, -0.0138, -0.0194,  ..., -0.0166,  0.0060, -0.0520]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.sam_model.state_dict()['blocks.0.attn.proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee3fe3b-95e8-4322-9df6-cca7c577a2f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T07:15:18.539034Z",
     "iopub.status.busy": "2025-11-06T07:15:18.538822Z",
     "iopub.status.idle": "2025-11-06T07:15:18.554001Z",
     "shell.execute_reply": "2025-11-06T07:15:18.553488Z",
     "shell.execute_reply.started": "2025-11-06T07:15:18.539018Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0572e-05, -1.6499e-04, -7.1049e-05,  ...,  4.5166e-03,\n",
       "         -2.9175e-02, -7.8201e-05],\n",
       "        [-1.3733e-04,  1.2159e-04,  4.2439e-05,  ..., -1.6632e-03,\n",
       "          3.1494e-02,  7.4387e-05],\n",
       "        [ 4.8065e-04,  7.7820e-04, -1.0967e-04,  ..., -1.6846e-02,\n",
       "          4.2969e-02,  1.5163e-04],\n",
       "        ...,\n",
       "        [ 2.1267e-04,  4.1008e-04, -7.2479e-05,  ...,  4.7913e-03,\n",
       "         -1.7319e-03, -6.6280e-05],\n",
       "        [ 3.0518e-04, -4.4346e-05, -2.2697e-04,  ...,  1.1536e-02,\n",
       "          3.3417e-03,  7.4863e-05],\n",
       "        [-2.8849e-05,  4.5967e-04,  9.3460e-05,  ..., -1.1292e-02,\n",
       "          3.7689e-03, -7.7724e-05]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.vision_model.state_dict()['transformer.layers.0.self_attn.qkv_proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9af448c-218e-447e-9773-0e7a8bdf7665",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-06T07:16:36.658906Z",
     "iopub.status.busy": "2025-11-06T07:16:36.658697Z",
     "iopub.status.idle": "2025-11-06T07:16:36.663907Z",
     "shell.execute_reply": "2025-11-06T07:16:36.663522Z",
     "shell.execute_reply.started": "2025-11-06T07:16:36.658891Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,    872,    198, 105043, 100165, 151645,    198, 151644,  77091,\n",
       "            198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"你是谁\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True, # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a977826-9396-4e88-a8c9-d83f8a30a505",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T07:16:38.766569Z",
     "iopub.status.busy": "2025-11-06T07:16:38.766361Z",
     "iopub.status.idle": "2025-11-06T07:16:55.294862Z",
     "shell.execute_reply": "2025-11-06T07:16:55.294348Z",
     "shell.execute_reply.started": "2025-11-06T07:16:38.766555Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: <think>\n",
      "嗯，用户问我是谁。我\n"
     ]
    }
   ],
   "source": [
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=10\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parse thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek-ocr-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
